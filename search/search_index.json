{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Optuna-ExternM Quick Start See the quick start example in the user guide. Installation Installing from source Clone the repository Text Only git clone http://github.com/trhallam/optuna-externm and install using pip Text Only cd optuna-externm pip install .","title":"About"},{"location":"index.html#optuna-externm","text":"","title":"Optuna-ExternM"},{"location":"index.html#quick-start","text":"See the quick start example in the user guide.","title":"Quick Start"},{"location":"index.html#installation","text":"","title":"Installation"},{"location":"index.html#installing-from-source","text":"Clone the repository Text Only git clone http://github.com/trhallam/optuna-externm and install using pip Text Only cd optuna-externm pip install .","title":"Installing from source"},{"location":"contact.html","text":"","title":"Contact"},{"location":"contrib.html","text":"","title":"Contributing"},{"location":"api/core.html","text":"optuna_externm.core BaseModelHandler Attributes: Name Type Description popt pd.DataFrame Optimisation Parameters Source code in optuna_externm/core/_bmh.py Python class BaseModelHandler : \"\"\" Attributes: popt (pd.DataFrame): Optimisation Parameters \"\"\" __maxfitness__ = 2000000000 __req_parameter_attr = set ( ( \"NAME\" , # index column should be PROP name \"LOW\" , # minimum search value for prop \"HIGH\" , # maximum search value for prop \"DEF\" , # default search value or initial guess \"DIST\" , # distribution function - if missing defaults to UNIFORM ) ) _equality_tokens = bidict ( { \"__lt__\" : \"<\" , \"__gt__\" : \">\" , \"__eq__\" : \"==\" , \"__le__\" : \"<=\" , \"__ge__\" : \">=\" , \"__ne__\" : \"!=\" , } ) _equality_tokenizer = re . compile ( r \"(==|!=|>=|<=|<|>)\" ) _optuna_dist_mapping = { \"UNIFORM\" : \"suggest_uniform\" , \"FLOAT\" : \"suggest_float\" , \"INT\" : \"suggest_int\" , \"LOGUNIFORM\" : \"suggest_loguniform\" , \"DISCRETE_UNIFORM\" : \"suggest_discrete_uniform\" , \"CATEGORICAL\" : \"suggest_categorical\" , } def __init__ ( self , results_dir = None ): \"\"\" Args: logger (str): The name to use for logging this handler. \"\"\" if results_dir is None : self . results_dir = pathlib . Path ( \".\" ) else : self . results_dir = pathlib . Path ( results_dir ) self . initialisers = OrderedDict () self . finalisers = OrderedDict () self . evaluators = OrderedDict () self . objectives = dict () self . transforms = dict () self . inverse_transforms = dict () self . set_finess = None self . evaluator_stats = list () self . log_keys = list () self . param = None self . param_list = list () self . param_cnstr = dict () self . nopt = 0 self . no_constraints = False self . no_transform = False def _register ( self , stage , alias , function , * args , ** kargs ): \"\"\"Genertic register function adds function to stage. Args: stage (str): accepted stages are ['initialisers', 'finalisers', 'evaluators'] \"\"\" pfunc = partial ( function , * args , ** kargs ) pfunc . __name__ = alias pfunc . __doc__ = function . __doc__ if hasattr ( function , \"__dict__\" ) and not isinstance ( function , type ): # Some functions don't have a dictionary, in these cases # simply don't copy it. Moreover, if the function is actually # a class, we do not want to copy the dictionary. pfunc . __dict__ . update ( function . __dict__ . copy ()) setattr ( self , alias , pfunc ) self . __dict__ [ stage ][ alias ] = self . __dict__ [ alias ] def register_evaluator ( self , alias , function , * args , ** kargs ): \"\"\"Register an evaluation *function* to the handler to be used for optimisation under the name *alias*. Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Args: alias (str): The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. function (func): The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. args/kwargs (optional) : Special arguments and keyword arguments to pass to function. Example: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console >>> def func(a, b, c=3): ... print a, b, c ... >>> model = BaseModelHandler() >>> model.register_evaluator(\"myFunc\", func, 2, c=4) >>> model.myFunc(3) 2 3 4 >>> model.evaluators {\"muFunc\": func} The registered function will be given the attributes :attr:`__name__` set to the fname and :attr:`__doc__` set to the original function's documentation. The :attr:`__dict__` attribute will also be updated with the original function's instance dictionary, if any. \"\"\" self . _register ( \"evaluators\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as evaluator\" ) def register_initialiser ( self , alias , function , * args , ** kargs ): \"\"\"Register an initialisation *function* to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"initialisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as initialiser\" ) def register_finaliser ( self , alias , function , * args , ** kargs ): \"\"\"Register a finalisation *function* to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"finalisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as finaliser\" ) def _unregister ( self , stage , alias ): \"\"\"Unregister *alias* from stage. Args: stage (str): The name of the stage to look for alias in. Valid stages are ['initisers', 'finalisers', 'evaluators'] alias (str): The name of the operator to remove from the model. \"\"\" _ = self . __dict__ [ stage ] . pop ( alias ) delattr ( self , alias ) def unregister_evaluator ( self , alias ): \"\"\"Unregister an evalautor function. Args: alias (str): The name of the operator to remove from the model. \"\"\" self . _unregister ( \"evaluators\" , alias ) logger . info ( f \" { alias } unregistered as evaluator\" ) def unregister_initialiser ( self , alias ): \"\"\"Unregister an initialiser function. Args: alias (str): The name of the initialiser function to remove from the model. \"\"\" self . _unregister ( \"initialisers\" , alias ) logger . info ( f \" { alias } unregistered as initisaliser\" ) def unregister_finaliser ( self , alias ): \"\"\"Unregister a finaliser function. Args: alias (str): The name of the finaliser function to remove from the model. \"\"\" self . _unregister ( \"finalisers\" , alias ) logger . info ( f \" { alias } unregistered as finaliser\" ) def unregister_transform ( self , alias ): \"\"\"Unregister a transform function. Args: alias (str): The name of the function to remove from the model. \"\"\" self . _unregister ( \"transforms\" , alias ) self . _unregister ( \"inverse_transforms\" , alias ) logger . info ( f \" { alias } unregistered as transform\" ) def set_param ( self , parameters ): \"\"\"Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Args: parameters: Optimisation parameters definitions as namedtuples. Notes: The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. \"\"\" missing_params = [] for param in parameters : if not self . __req_parameter_attr . issubset ( set ( param . _fields )): missing_params . append ( set ( param . _fields ) . difference ( self . __req_parameter_attr ) ) logger . debug ( f \"Malformed parameter: { param } \" ) if missing_params : logger . error ( f \"The parameter definition was malformed, missing: { missing_params } \" ) raise SystemExit # if \"DIST\" not in param.columns: # param[\"DIST\"] = \"UNIFORM\" # logger.info( # \"No parameter distribution specified, all parameters \" # \"being transformed with UNIFORM distribution\" # ) # if \"TYPE\" not in param.columns: # param[\"TYPE\"] = \"NUMERIC\" # logger.info( # \"No parameter distribution types specified, all parameters \" # \"assumed to have type NUMERIC.\" # ) # # dealing with parameter constraints # if \"CONSTRAINT\" not in param.columns or self.no_constraints: # param[\"CONSTRAINT\"] = numpy.nan # else: # for par, row in param.dropna(subset=[\"CONSTRAINT\"]).iterrows(): # tok = self._equality_tokenizer.split(row[\"CONSTRAINT\"]) # tok = [t.strip() for t in tok if t != \"\"] # for i, t in enumerate(tok): # try: # tok[i] = self._equality_tokens.inverse[t][0] # except KeyError: # pass # self.param_cnstr[par] = [t for t in zip(tok[0::2], tok[1::2])] self . parameters = parameters self . param_list = [ p . NAME for p in self . parameters ] self . nopt = len ( self . parameters ) logger . info ( f \"Model has { self . nopt } optimisation parameters\" ) def initialise ( self ): \"\"\"Initialise the model by calling functions registered for initialisation.\"\"\" for init in self . initialisers : self . initialisers [ init ]() def finalise ( self ): \"\"\"Finalise the model by calling functions registered for finalisation.\"\"\" for fini in self . finalisers : self . finalisers [ fini ]() def evaluate ( self , results = None ): \"\"\"Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Args: results (dict): The dictionary to pass between evaluators. Returns: (dict): Updated results dictionary. \"\"\" if results is None : results = dict () for evl in self . evaluators : results . update ( self . evaluators [ evl ]( results = results )) return results def log_values ( self , log_list = None ): \"\"\"Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Arguments: log_list (list): List of results keys (str) to report on. \"\"\" # 'fit' will always be reported. Defaults to None: Only fit will # be reported. # In multi-objective cases the optimiser will reduce fit of tuple to # fit1, fit2, ..., fitn if log_list is not None : self . log_keys = list ({ \"fit\" } . union ( set ( log_list ))) self . evaluator_stats = list ({ \"fit\" } . union ( set ( log_list ))) else : self . log_keys = [ \"fit\" ] self . evaluator_stats = [ \"fit\" ] def objective_function ( self ): \"\"\"Returns the final objective function for Optuna Studies\"\"\" def objective ( trial ): p = { param . NAME : getattr ( trial , self . _optuna_dist_mapping [ param . DIST ])( param . NAME , param . LOW , param . HIGH ) for param in self . parameters } res = { \"p\" : p , \"trial_number\" : trial . number } res . update ( trial . study . user_attrs ) results = self . evaluate ( results = res ) for key in self . log_keys : try : trial . set_user_attr ( key , results [ key ]) except KeyError : pass return results [ \"fit\" ] return objective __init__ ( self , results_dir = None ) special Parameters: Name Type Description Default logger str The name to use for logging this handler. required Source code in optuna_externm/core/_bmh.py Python def __init__ ( self , results_dir = None ): \"\"\" Args: logger (str): The name to use for logging this handler. \"\"\" if results_dir is None : self . results_dir = pathlib . Path ( \".\" ) else : self . results_dir = pathlib . Path ( results_dir ) self . initialisers = OrderedDict () self . finalisers = OrderedDict () self . evaluators = OrderedDict () self . objectives = dict () self . transforms = dict () self . inverse_transforms = dict () self . set_finess = None self . evaluator_stats = list () self . log_keys = list () self . param = None self . param_list = list () self . param_cnstr = dict () self . nopt = 0 self . no_constraints = False self . no_transform = False evaluate ( self , results = None ) Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Parameters: Name Type Description Default results dict The dictionary to pass between evaluators. None Returns: Type Description (dict) Updated results dictionary. Source code in optuna_externm/core/_bmh.py Python def evaluate ( self , results = None ): \"\"\"Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Args: results (dict): The dictionary to pass between evaluators. Returns: (dict): Updated results dictionary. \"\"\" if results is None : results = dict () for evl in self . evaluators : results . update ( self . evaluators [ evl ]( results = results )) return results finalise ( self ) Finalise the model by calling functions registered for finalisation. Source code in optuna_externm/core/_bmh.py Python def finalise ( self ): \"\"\"Finalise the model by calling functions registered for finalisation.\"\"\" for fini in self . finalisers : self . finalisers [ fini ]() initialise ( self ) Initialise the model by calling functions registered for initialisation. Source code in optuna_externm/core/_bmh.py Python def initialise ( self ): \"\"\"Initialise the model by calling functions registered for initialisation.\"\"\" for init in self . initialisers : self . initialisers [ init ]() log_values ( self , log_list = None ) Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Parameters: Name Type Description Default log_list list List of results keys (str) to report on. None Source code in optuna_externm/core/_bmh.py Python def log_values ( self , log_list = None ): \"\"\"Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Arguments: log_list (list): List of results keys (str) to report on. \"\"\" # 'fit' will always be reported. Defaults to None: Only fit will # be reported. # In multi-objective cases the optimiser will reduce fit of tuple to # fit1, fit2, ..., fitn if log_list is not None : self . log_keys = list ({ \"fit\" } . union ( set ( log_list ))) self . evaluator_stats = list ({ \"fit\" } . union ( set ( log_list ))) else : self . log_keys = [ \"fit\" ] self . evaluator_stats = [ \"fit\" ] objective_function ( self ) Returns the final objective function for Optuna Studies Source code in optuna_externm/core/_bmh.py Python def objective_function ( self ): \"\"\"Returns the final objective function for Optuna Studies\"\"\" def objective ( trial ): p = { param . NAME : getattr ( trial , self . _optuna_dist_mapping [ param . DIST ])( param . NAME , param . LOW , param . HIGH ) for param in self . parameters } res = { \"p\" : p , \"trial_number\" : trial . number } res . update ( trial . study . user_attrs ) results = self . evaluate ( results = res ) for key in self . log_keys : try : trial . set_user_attr ( key , results [ key ]) except KeyError : pass return results [ \"fit\" ] return objective register_evaluator ( self , alias , function , * args , ** kargs ) Register an evaluation function to the handler to be used for optimisation under the name alias . Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Parameters: Name Type Description Default alias str The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. required function func The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. required args/kwargs optional) Special arguments and keyword arguments to pass to function. required Examples: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console Python >>> def func ( a , b , c = 3 ): ... print a , b , c ... >>> model = BaseModelHandler () >>> model . register_evaluator ( \"myFunc\" , func , 2 , c = 4 ) >>> model . myFunc ( 3 ) 2 3 4 >>> model . evaluators { \"muFunc\" : func } The registered function will be given the attributes :attr: __name__ set to the fname and :attr: __doc__ set to the original function's documentation. The :attr: __dict__ attribute will also be updated with the original function's instance dictionary, if any. Source code in optuna_externm/core/_bmh.py Python def register_evaluator ( self , alias , function , * args , ** kargs ): \"\"\"Register an evaluation *function* to the handler to be used for optimisation under the name *alias*. Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Args: alias (str): The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. function (func): The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. args/kwargs (optional) : Special arguments and keyword arguments to pass to function. Example: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console >>> def func(a, b, c=3): ... print a, b, c ... >>> model = BaseModelHandler() >>> model.register_evaluator(\"myFunc\", func, 2, c=4) >>> model.myFunc(3) 2 3 4 >>> model.evaluators {\"muFunc\": func} The registered function will be given the attributes :attr:`__name__` set to the fname and :attr:`__doc__` set to the original function's documentation. The :attr:`__dict__` attribute will also be updated with the original function's instance dictionary, if any. \"\"\" self . _register ( \"evaluators\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as evaluator\" ) register_finaliser ( self , alias , function , * args , ** kargs ) Register a finalisation function to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator Source code in optuna_externm/core/_bmh.py Python def register_finaliser ( self , alias , function , * args , ** kargs ): \"\"\"Register a finalisation *function* to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"finalisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as finaliser\" ) register_initialiser ( self , alias , function , * args , ** kargs ) Register an initialisation function to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator Source code in optuna_externm/core/_bmh.py Python def register_initialiser ( self , alias , function , * args , ** kargs ): \"\"\"Register an initialisation *function* to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"initialisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as initialiser\" ) set_param ( self , parameters ) Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Parameters: Name Type Description Default parameters Optimisation parameters definitions as namedtuples. required Notes The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. Source code in optuna_externm/core/_bmh.py Python def set_param ( self , parameters ): \"\"\"Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Args: parameters: Optimisation parameters definitions as namedtuples. Notes: The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. \"\"\" missing_params = [] for param in parameters : if not self . __req_parameter_attr . issubset ( set ( param . _fields )): missing_params . append ( set ( param . _fields ) . difference ( self . __req_parameter_attr ) ) logger . debug ( f \"Malformed parameter: { param } \" ) if missing_params : logger . error ( f \"The parameter definition was malformed, missing: { missing_params } \" ) raise SystemExit # if \"DIST\" not in param.columns: # param[\"DIST\"] = \"UNIFORM\" # logger.info( # \"No parameter distribution specified, all parameters \" # \"being transformed with UNIFORM distribution\" # ) # if \"TYPE\" not in param.columns: # param[\"TYPE\"] = \"NUMERIC\" # logger.info( # \"No parameter distribution types specified, all parameters \" # \"assumed to have type NUMERIC.\" # ) # # dealing with parameter constraints # if \"CONSTRAINT\" not in param.columns or self.no_constraints: # param[\"CONSTRAINT\"] = numpy.nan # else: # for par, row in param.dropna(subset=[\"CONSTRAINT\"]).iterrows(): # tok = self._equality_tokenizer.split(row[\"CONSTRAINT\"]) # tok = [t.strip() for t in tok if t != \"\"] # for i, t in enumerate(tok): # try: # tok[i] = self._equality_tokens.inverse[t][0] # except KeyError: # pass # self.param_cnstr[par] = [t for t in zip(tok[0::2], tok[1::2])] self . parameters = parameters self . param_list = [ p . NAME for p in self . parameters ] self . nopt = len ( self . parameters ) logger . info ( f \"Model has { self . nopt } optimisation parameters\" ) unregister_evaluator ( self , alias ) Unregister an evalautor function. Parameters: Name Type Description Default alias str The name of the operator to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_evaluator ( self , alias ): \"\"\"Unregister an evalautor function. Args: alias (str): The name of the operator to remove from the model. \"\"\" self . _unregister ( \"evaluators\" , alias ) logger . info ( f \" { alias } unregistered as evaluator\" ) unregister_finaliser ( self , alias ) Unregister a finaliser function. Parameters: Name Type Description Default alias str The name of the finaliser function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_finaliser ( self , alias ): \"\"\"Unregister a finaliser function. Args: alias (str): The name of the finaliser function to remove from the model. \"\"\" self . _unregister ( \"finalisers\" , alias ) logger . info ( f \" { alias } unregistered as finaliser\" ) unregister_initialiser ( self , alias ) Unregister an initialiser function. Parameters: Name Type Description Default alias str The name of the initialiser function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_initialiser ( self , alias ): \"\"\"Unregister an initialiser function. Args: alias (str): The name of the initialiser function to remove from the model. \"\"\" self . _unregister ( \"initialisers\" , alias ) logger . info ( f \" { alias } unregistered as initisaliser\" ) unregister_transform ( self , alias ) Unregister a transform function. Parameters: Name Type Description Default alias str The name of the function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_transform ( self , alias ): \"\"\"Unregister a transform function. Args: alias (str): The name of the function to remove from the model. \"\"\" self . _unregister ( \"transforms\" , alias ) self . _unregister ( \"inverse_transforms\" , alias ) logger . info ( f \" { alias } unregistered as transform\" )","title":"Core"},{"location":"api/core.html#optuna_externmcore","text":"","title":"optuna_externm.core"},{"location":"api/core.html#basemodelhandler","text":"Attributes: Name Type Description popt pd.DataFrame Optimisation Parameters Source code in optuna_externm/core/_bmh.py Python class BaseModelHandler : \"\"\" Attributes: popt (pd.DataFrame): Optimisation Parameters \"\"\" __maxfitness__ = 2000000000 __req_parameter_attr = set ( ( \"NAME\" , # index column should be PROP name \"LOW\" , # minimum search value for prop \"HIGH\" , # maximum search value for prop \"DEF\" , # default search value or initial guess \"DIST\" , # distribution function - if missing defaults to UNIFORM ) ) _equality_tokens = bidict ( { \"__lt__\" : \"<\" , \"__gt__\" : \">\" , \"__eq__\" : \"==\" , \"__le__\" : \"<=\" , \"__ge__\" : \">=\" , \"__ne__\" : \"!=\" , } ) _equality_tokenizer = re . compile ( r \"(==|!=|>=|<=|<|>)\" ) _optuna_dist_mapping = { \"UNIFORM\" : \"suggest_uniform\" , \"FLOAT\" : \"suggest_float\" , \"INT\" : \"suggest_int\" , \"LOGUNIFORM\" : \"suggest_loguniform\" , \"DISCRETE_UNIFORM\" : \"suggest_discrete_uniform\" , \"CATEGORICAL\" : \"suggest_categorical\" , } def __init__ ( self , results_dir = None ): \"\"\" Args: logger (str): The name to use for logging this handler. \"\"\" if results_dir is None : self . results_dir = pathlib . Path ( \".\" ) else : self . results_dir = pathlib . Path ( results_dir ) self . initialisers = OrderedDict () self . finalisers = OrderedDict () self . evaluators = OrderedDict () self . objectives = dict () self . transforms = dict () self . inverse_transforms = dict () self . set_finess = None self . evaluator_stats = list () self . log_keys = list () self . param = None self . param_list = list () self . param_cnstr = dict () self . nopt = 0 self . no_constraints = False self . no_transform = False def _register ( self , stage , alias , function , * args , ** kargs ): \"\"\"Genertic register function adds function to stage. Args: stage (str): accepted stages are ['initialisers', 'finalisers', 'evaluators'] \"\"\" pfunc = partial ( function , * args , ** kargs ) pfunc . __name__ = alias pfunc . __doc__ = function . __doc__ if hasattr ( function , \"__dict__\" ) and not isinstance ( function , type ): # Some functions don't have a dictionary, in these cases # simply don't copy it. Moreover, if the function is actually # a class, we do not want to copy the dictionary. pfunc . __dict__ . update ( function . __dict__ . copy ()) setattr ( self , alias , pfunc ) self . __dict__ [ stage ][ alias ] = self . __dict__ [ alias ] def register_evaluator ( self , alias , function , * args , ** kargs ): \"\"\"Register an evaluation *function* to the handler to be used for optimisation under the name *alias*. Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Args: alias (str): The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. function (func): The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. args/kwargs (optional) : Special arguments and keyword arguments to pass to function. Example: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console >>> def func(a, b, c=3): ... print a, b, c ... >>> model = BaseModelHandler() >>> model.register_evaluator(\"myFunc\", func, 2, c=4) >>> model.myFunc(3) 2 3 4 >>> model.evaluators {\"muFunc\": func} The registered function will be given the attributes :attr:`__name__` set to the fname and :attr:`__doc__` set to the original function's documentation. The :attr:`__dict__` attribute will also be updated with the original function's instance dictionary, if any. \"\"\" self . _register ( \"evaluators\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as evaluator\" ) def register_initialiser ( self , alias , function , * args , ** kargs ): \"\"\"Register an initialisation *function* to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"initialisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as initialiser\" ) def register_finaliser ( self , alias , function , * args , ** kargs ): \"\"\"Register a finalisation *function* to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"finalisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as finaliser\" ) def _unregister ( self , stage , alias ): \"\"\"Unregister *alias* from stage. Args: stage (str): The name of the stage to look for alias in. Valid stages are ['initisers', 'finalisers', 'evaluators'] alias (str): The name of the operator to remove from the model. \"\"\" _ = self . __dict__ [ stage ] . pop ( alias ) delattr ( self , alias ) def unregister_evaluator ( self , alias ): \"\"\"Unregister an evalautor function. Args: alias (str): The name of the operator to remove from the model. \"\"\" self . _unregister ( \"evaluators\" , alias ) logger . info ( f \" { alias } unregistered as evaluator\" ) def unregister_initialiser ( self , alias ): \"\"\"Unregister an initialiser function. Args: alias (str): The name of the initialiser function to remove from the model. \"\"\" self . _unregister ( \"initialisers\" , alias ) logger . info ( f \" { alias } unregistered as initisaliser\" ) def unregister_finaliser ( self , alias ): \"\"\"Unregister a finaliser function. Args: alias (str): The name of the finaliser function to remove from the model. \"\"\" self . _unregister ( \"finalisers\" , alias ) logger . info ( f \" { alias } unregistered as finaliser\" ) def unregister_transform ( self , alias ): \"\"\"Unregister a transform function. Args: alias (str): The name of the function to remove from the model. \"\"\" self . _unregister ( \"transforms\" , alias ) self . _unregister ( \"inverse_transforms\" , alias ) logger . info ( f \" { alias } unregistered as transform\" ) def set_param ( self , parameters ): \"\"\"Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Args: parameters: Optimisation parameters definitions as namedtuples. Notes: The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. \"\"\" missing_params = [] for param in parameters : if not self . __req_parameter_attr . issubset ( set ( param . _fields )): missing_params . append ( set ( param . _fields ) . difference ( self . __req_parameter_attr ) ) logger . debug ( f \"Malformed parameter: { param } \" ) if missing_params : logger . error ( f \"The parameter definition was malformed, missing: { missing_params } \" ) raise SystemExit # if \"DIST\" not in param.columns: # param[\"DIST\"] = \"UNIFORM\" # logger.info( # \"No parameter distribution specified, all parameters \" # \"being transformed with UNIFORM distribution\" # ) # if \"TYPE\" not in param.columns: # param[\"TYPE\"] = \"NUMERIC\" # logger.info( # \"No parameter distribution types specified, all parameters \" # \"assumed to have type NUMERIC.\" # ) # # dealing with parameter constraints # if \"CONSTRAINT\" not in param.columns or self.no_constraints: # param[\"CONSTRAINT\"] = numpy.nan # else: # for par, row in param.dropna(subset=[\"CONSTRAINT\"]).iterrows(): # tok = self._equality_tokenizer.split(row[\"CONSTRAINT\"]) # tok = [t.strip() for t in tok if t != \"\"] # for i, t in enumerate(tok): # try: # tok[i] = self._equality_tokens.inverse[t][0] # except KeyError: # pass # self.param_cnstr[par] = [t for t in zip(tok[0::2], tok[1::2])] self . parameters = parameters self . param_list = [ p . NAME for p in self . parameters ] self . nopt = len ( self . parameters ) logger . info ( f \"Model has { self . nopt } optimisation parameters\" ) def initialise ( self ): \"\"\"Initialise the model by calling functions registered for initialisation.\"\"\" for init in self . initialisers : self . initialisers [ init ]() def finalise ( self ): \"\"\"Finalise the model by calling functions registered for finalisation.\"\"\" for fini in self . finalisers : self . finalisers [ fini ]() def evaluate ( self , results = None ): \"\"\"Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Args: results (dict): The dictionary to pass between evaluators. Returns: (dict): Updated results dictionary. \"\"\" if results is None : results = dict () for evl in self . evaluators : results . update ( self . evaluators [ evl ]( results = results )) return results def log_values ( self , log_list = None ): \"\"\"Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Arguments: log_list (list): List of results keys (str) to report on. \"\"\" # 'fit' will always be reported. Defaults to None: Only fit will # be reported. # In multi-objective cases the optimiser will reduce fit of tuple to # fit1, fit2, ..., fitn if log_list is not None : self . log_keys = list ({ \"fit\" } . union ( set ( log_list ))) self . evaluator_stats = list ({ \"fit\" } . union ( set ( log_list ))) else : self . log_keys = [ \"fit\" ] self . evaluator_stats = [ \"fit\" ] def objective_function ( self ): \"\"\"Returns the final objective function for Optuna Studies\"\"\" def objective ( trial ): p = { param . NAME : getattr ( trial , self . _optuna_dist_mapping [ param . DIST ])( param . NAME , param . LOW , param . HIGH ) for param in self . parameters } res = { \"p\" : p , \"trial_number\" : trial . number } res . update ( trial . study . user_attrs ) results = self . evaluate ( results = res ) for key in self . log_keys : try : trial . set_user_attr ( key , results [ key ]) except KeyError : pass return results [ \"fit\" ] return objective","title":"BaseModelHandler"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.__init__","text":"Parameters: Name Type Description Default logger str The name to use for logging this handler. required Source code in optuna_externm/core/_bmh.py Python def __init__ ( self , results_dir = None ): \"\"\" Args: logger (str): The name to use for logging this handler. \"\"\" if results_dir is None : self . results_dir = pathlib . Path ( \".\" ) else : self . results_dir = pathlib . Path ( results_dir ) self . initialisers = OrderedDict () self . finalisers = OrderedDict () self . evaluators = OrderedDict () self . objectives = dict () self . transforms = dict () self . inverse_transforms = dict () self . set_finess = None self . evaluator_stats = list () self . log_keys = list () self . param = None self . param_list = list () self . param_cnstr = dict () self . nopt = 0 self . no_constraints = False self . no_transform = False","title":"__init__()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.evaluate","text":"Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Parameters: Name Type Description Default results dict The dictionary to pass between evaluators. None Returns: Type Description (dict) Updated results dictionary. Source code in optuna_externm/core/_bmh.py Python def evaluate ( self , results = None ): \"\"\"Run the evaluation functions. Usually this is performed by the optimiser. Use this function to run tests on your model or to perform debugging. Args: results (dict): The dictionary to pass between evaluators. Returns: (dict): Updated results dictionary. \"\"\" if results is None : results = dict () for evl in self . evaluators : results . update ( self . evaluators [ evl ]( results = results )) return results","title":"evaluate()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.finalise","text":"Finalise the model by calling functions registered for finalisation. Source code in optuna_externm/core/_bmh.py Python def finalise ( self ): \"\"\"Finalise the model by calling functions registered for finalisation.\"\"\" for fini in self . finalisers : self . finalisers [ fini ]()","title":"finalise()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.initialise","text":"Initialise the model by calling functions registered for initialisation. Source code in optuna_externm/core/_bmh.py Python def initialise ( self ): \"\"\"Initialise the model by calling functions registered for initialisation.\"\"\" for init in self . initialisers : self . initialisers [ init ]()","title":"initialise()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.log_values","text":"Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Parameters: Name Type Description Default log_list list List of results keys (str) to report on. None Source code in optuna_externm/core/_bmh.py Python def log_values ( self , log_list = None ): \"\"\"Select values to log for each trial. These must be keys available within your results parser between evaluators by the end of each individuals evaluation. They should also be simple types. Arguments: log_list (list): List of results keys (str) to report on. \"\"\" # 'fit' will always be reported. Defaults to None: Only fit will # be reported. # In multi-objective cases the optimiser will reduce fit of tuple to # fit1, fit2, ..., fitn if log_list is not None : self . log_keys = list ({ \"fit\" } . union ( set ( log_list ))) self . evaluator_stats = list ({ \"fit\" } . union ( set ( log_list ))) else : self . log_keys = [ \"fit\" ] self . evaluator_stats = [ \"fit\" ]","title":"log_values()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.objective_function","text":"Returns the final objective function for Optuna Studies Source code in optuna_externm/core/_bmh.py Python def objective_function ( self ): \"\"\"Returns the final objective function for Optuna Studies\"\"\" def objective ( trial ): p = { param . NAME : getattr ( trial , self . _optuna_dist_mapping [ param . DIST ])( param . NAME , param . LOW , param . HIGH ) for param in self . parameters } res = { \"p\" : p , \"trial_number\" : trial . number } res . update ( trial . study . user_attrs ) results = self . evaluate ( results = res ) for key in self . log_keys : try : trial . set_user_attr ( key , results [ key ]) except KeyError : pass return results [ \"fit\" ] return objective","title":"objective_function()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.register_evaluator","text":"Register an evaluation function to the handler to be used for optimisation under the name alias . Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Parameters: Name Type Description Default alias str The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. required function func The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. required args/kwargs optional) Special arguments and keyword arguments to pass to function. required Examples: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console Python >>> def func ( a , b , c = 3 ): ... print a , b , c ... >>> model = BaseModelHandler () >>> model . register_evaluator ( \"myFunc\" , func , 2 , c = 4 ) >>> model . myFunc ( 3 ) 2 3 4 >>> model . evaluators { \"muFunc\" : func } The registered function will be given the attributes :attr: __name__ set to the fname and :attr: __doc__ set to the original function's documentation. The :attr: __dict__ attribute will also be updated with the original function's instance dictionary, if any. Source code in optuna_externm/core/_bmh.py Python def register_evaluator ( self , alias , function , * args , ** kargs ): \"\"\"Register an evaluation *function* to the handler to be used for optimisation under the name *alias*. Default arguments will be passed automatically when calling the registered function. Fixed arguments can then be overriden at function call time. All of the registered evaluators will be used in the objective updating function in the order that they are registered. This is important if evaluators depend on the output of other evaluators. Original code and method inspiration from DEAP.Toolbox Args: alias (str): The name the function will have in the handler. If the fname already exist it will overwrite the the function already registered. function (func): The function referred to by alias, the first argument is the reference data or a function to return the reference data, the second argument is the data to compare. args/kwargs (optional) : Special arguments and keyword arguments to pass to function. Example: The following code block is an example of how this function is used. .. highlight:: python .. code-block:: console >>> def func(a, b, c=3): ... print a, b, c ... >>> model = BaseModelHandler() >>> model.register_evaluator(\"myFunc\", func, 2, c=4) >>> model.myFunc(3) 2 3 4 >>> model.evaluators {\"muFunc\": func} The registered function will be given the attributes :attr:`__name__` set to the fname and :attr:`__doc__` set to the original function's documentation. The :attr:`__dict__` attribute will also be updated with the original function's instance dictionary, if any. \"\"\" self . _register ( \"evaluators\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as evaluator\" )","title":"register_evaluator()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.register_finaliser","text":"Register a finalisation function to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator Source code in optuna_externm/core/_bmh.py Python def register_finaliser ( self , alias , function , * args , ** kargs ): \"\"\"Register a finalisation *function* to the handler to be used after to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"finalisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as finaliser\" )","title":"register_finaliser()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.register_initialiser","text":"Register an initialisation function to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator Source code in optuna_externm/core/_bmh.py Python def register_initialiser ( self , alias , function , * args , ** kargs ): \"\"\"Register an initialisation *function* to the handler to be used prior to running the evaluators. These functions are run once. For usage see BaseModelHander.register_evalator \"\"\" self . _register ( \"initialisers\" , alias , function , * args , ** kargs ) logger . info ( f \" { alias } registered as initialiser\" )","title":"register_initialiser()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.set_param","text":"Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Parameters: Name Type Description Default parameters Optimisation parameters definitions as namedtuples. required Notes The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. Source code in optuna_externm/core/_bmh.py Python def set_param ( self , parameters ): \"\"\"Set the parameters of the problem to be solved. Minimum properties for each tuples are PROP, PMIN, PMAX, DIST Args: parameters: Optimisation parameters definitions as namedtuples. Notes: The load csv function in the optuna.externm.utils module can be used to load and create the input to this method from a CSV file. \"\"\" missing_params = [] for param in parameters : if not self . __req_parameter_attr . issubset ( set ( param . _fields )): missing_params . append ( set ( param . _fields ) . difference ( self . __req_parameter_attr ) ) logger . debug ( f \"Malformed parameter: { param } \" ) if missing_params : logger . error ( f \"The parameter definition was malformed, missing: { missing_params } \" ) raise SystemExit # if \"DIST\" not in param.columns: # param[\"DIST\"] = \"UNIFORM\" # logger.info( # \"No parameter distribution specified, all parameters \" # \"being transformed with UNIFORM distribution\" # ) # if \"TYPE\" not in param.columns: # param[\"TYPE\"] = \"NUMERIC\" # logger.info( # \"No parameter distribution types specified, all parameters \" # \"assumed to have type NUMERIC.\" # ) # # dealing with parameter constraints # if \"CONSTRAINT\" not in param.columns or self.no_constraints: # param[\"CONSTRAINT\"] = numpy.nan # else: # for par, row in param.dropna(subset=[\"CONSTRAINT\"]).iterrows(): # tok = self._equality_tokenizer.split(row[\"CONSTRAINT\"]) # tok = [t.strip() for t in tok if t != \"\"] # for i, t in enumerate(tok): # try: # tok[i] = self._equality_tokens.inverse[t][0] # except KeyError: # pass # self.param_cnstr[par] = [t for t in zip(tok[0::2], tok[1::2])] self . parameters = parameters self . param_list = [ p . NAME for p in self . parameters ] self . nopt = len ( self . parameters ) logger . info ( f \"Model has { self . nopt } optimisation parameters\" )","title":"set_param()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.unregister_evaluator","text":"Unregister an evalautor function. Parameters: Name Type Description Default alias str The name of the operator to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_evaluator ( self , alias ): \"\"\"Unregister an evalautor function. Args: alias (str): The name of the operator to remove from the model. \"\"\" self . _unregister ( \"evaluators\" , alias ) logger . info ( f \" { alias } unregistered as evaluator\" )","title":"unregister_evaluator()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.unregister_finaliser","text":"Unregister a finaliser function. Parameters: Name Type Description Default alias str The name of the finaliser function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_finaliser ( self , alias ): \"\"\"Unregister a finaliser function. Args: alias (str): The name of the finaliser function to remove from the model. \"\"\" self . _unregister ( \"finalisers\" , alias ) logger . info ( f \" { alias } unregistered as finaliser\" )","title":"unregister_finaliser()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.unregister_initialiser","text":"Unregister an initialiser function. Parameters: Name Type Description Default alias str The name of the initialiser function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_initialiser ( self , alias ): \"\"\"Unregister an initialiser function. Args: alias (str): The name of the initialiser function to remove from the model. \"\"\" self . _unregister ( \"initialisers\" , alias ) logger . info ( f \" { alias } unregistered as initisaliser\" )","title":"unregister_initialiser()"},{"location":"api/core.html#optuna_externm.core._bmh.BaseModelHandler.unregister_transform","text":"Unregister a transform function. Parameters: Name Type Description Default alias str The name of the function to remove from the model. required Source code in optuna_externm/core/_bmh.py Python def unregister_transform ( self , alias ): \"\"\"Unregister a transform function. Args: alias (str): The name of the function to remove from the model. \"\"\" self . _unregister ( \"transforms\" , alias ) self . _unregister ( \"inverse_transforms\" , alias ) logger . info ( f \" { alias } unregistered as transform\" )","title":"unregister_transform()"},{"location":"cli/cli.html","text":"optuna-em gc Usage gc help menu To see the help menu Bash optuna-em gc --help Which has output: Bash Usage: optuna-em gc [ OPTIONS ] Generate a default yaml config file. Options: -f, --fname TEXT --help Show this message and exit. optuna-em results Usage results help menu To see the help menu Bash optuna-em results --help Which has output: Bash Usage: optuna-em results [ OPTIONS ] CONFIG_FILE Options: -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit. optuna-em opt Usage opt help menu To see the help menu Bash optuna-em opt --help Which has output: Bash Usage: optuna-em opt [ OPTIONS ] CONFIG_FILE Process a configurartion file and run an optimisation study. Options: -j, --jobs INTEGER RANGE Number of parallel processors to use for running model evaluations. [ 1 < = x< = 4 ] -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit. A default CONFIG_FILE can be generated by Bash optuna-em gc opt optuna-em opt Config OPTUNA section parameters Parameters for controlling Optuna bmh_function_name : str pydantic-field The name of the function which returns the BaseModelHandler to be evaluated. direction : _DirectionEnum pydantic-field The optimisation direction (minimize/maximise), if using Multi-Objective use 'directions' parameters. directions : List [ optuna_externm . config . _DirectionEnum ] pydantic-field The optimisation direction (minimize/maximise), if using Multi-Objective use 'directions' parameters. n_trials : int pydantic-field The number of trials that should be run. opt_model_file : Path pydantic-field The optimisation model python file. optuna_sqldb : str pydantic-field Adress of Optuna SQL database for studies. optuna_study_name : str pydantic-field The name of the Optuna study - SQL ID sampler : str pydantic-field The sampler to use with Optuna. A section with the sampler name will be required for sampler specific parameters. USER section parameters Put paramters here that will be exposed to the BaseModelHandler user_example : str pydantic-field An example user parameter comment. Example Configuration YAML OPTUNA : optuna_sqldb : '' optuna_study_name : '' sampler : nsgaii direction : ~ directions : ~ opt_model_file : '' bmh_function_name : setup n_trials : 1 USER : user_example : example_user_paramter","title":"CLI"},{"location":"cli/cli.html#optuna-em-gc-usage","text":"","title":"optuna-em gc Usage"},{"location":"cli/cli.html#gc-help-menu","text":"To see the help menu Bash optuna-em gc --help Which has output: Bash Usage: optuna-em gc [ OPTIONS ] Generate a default yaml config file. Options: -f, --fname TEXT --help Show this message and exit.","title":"gc help menu"},{"location":"cli/cli.html#optuna-em-results-usage","text":"","title":"optuna-em results Usage"},{"location":"cli/cli.html#results-help-menu","text":"To see the help menu Bash optuna-em results --help Which has output: Bash Usage: optuna-em results [ OPTIONS ] CONFIG_FILE Options: -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit.","title":"results help menu"},{"location":"cli/cli.html#optuna-em-opt-usage","text":"","title":"optuna-em opt Usage"},{"location":"cli/cli.html#opt-help-menu","text":"To see the help menu Bash optuna-em opt --help Which has output: Bash Usage: optuna-em opt [ OPTIONS ] CONFIG_FILE Process a configurartion file and run an optimisation study. Options: -j, --jobs INTEGER RANGE Number of parallel processors to use for running model evaluations. [ 1 < = x< = 4 ] -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit. A default CONFIG_FILE can be generated by Bash optuna-em gc opt","title":"opt help menu"},{"location":"cli/cli.html#optuna-em-opt-config","text":"","title":"optuna-em opt Config"},{"location":"cli/cli.html#optuna-section-parameters","text":"Parameters for controlling Optuna","title":"OPTUNA section parameters"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.bmh_function_name","text":"The name of the function which returns the BaseModelHandler to be evaluated.","title":"bmh_function_name"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.direction","text":"The optimisation direction (minimize/maximise), if using Multi-Objective use 'directions' parameters.","title":"direction"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.directions","text":"The optimisation direction (minimize/maximise), if using Multi-Objective use 'directions' parameters.","title":"directions"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.n_trials","text":"The number of trials that should be run.","title":"n_trials"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.opt_model_file","text":"The optimisation model python file.","title":"opt_model_file"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.optuna_sqldb","text":"Adress of Optuna SQL database for studies.","title":"optuna_sqldb"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.optuna_study_name","text":"The name of the Optuna study - SQL ID","title":"optuna_study_name"},{"location":"cli/cli.html#optuna_externm.config._OptunaConfig.sampler","text":"The sampler to use with Optuna. A section with the sampler name will be required for sampler specific parameters.","title":"sampler"},{"location":"cli/cli.html#user-section-parameters","text":"Put paramters here that will be exposed to the BaseModelHandler","title":"USER section parameters"},{"location":"cli/cli.html#optuna_externm.config._UserConfig.user_example","text":"An example user parameter comment.","title":"user_example"},{"location":"cli/cli.html#example-configuration","text":"YAML OPTUNA : optuna_sqldb : '' optuna_study_name : '' sampler : nsgaii direction : ~ directions : ~ opt_model_file : '' bmh_function_name : setup n_trials : 1 USER : user_example : example_user_paramter","title":"Example Configuration"},{"location":"cli/help_gc.html","text":"To see the help menu Bash optuna-em gc --help Which has output: Bash Usage: optuna-em gc [ OPTIONS ] Generate a default yaml config file. Options: -f, --fname TEXT --help Show this message and exit.","title":"Help gc"},{"location":"cli/help_opt.html","text":"To see the help menu Bash optuna-em opt --help Which has output: Bash Usage: optuna-em opt [ OPTIONS ] CONFIG_FILE Process a configurartion file and run an optimisation study. Options: -j, --jobs INTEGER RANGE Number of parallel processors to use for running model evaluations. [ 1 < = x< = 4 ] -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit. A default CONFIG_FILE can be generated by Bash optuna-em gc opt","title":"Help opt"},{"location":"cli/help_results.html","text":"To see the help menu Bash optuna-em results --help Which has output: Bash Usage: optuna-em results [ OPTIONS ] CONFIG_FILE Options: -d, --debug BOOLEAN Activate debugging output. --help Show this message and exit.","title":"Help results"},{"location":"ophm/about.html","text":"","title":"About"},{"location":"ophm/api_handlers.html","text":"Handlers Python from optuna_hm import handlers optuna_hm.handlers.ecl Handlers for interfacing with and evaluating Eclipse (Schlumberger) Reservoir Simulation models. Eclipse is a proprietary simulation software which has a command line interface. Handler module for external file and process handling of ECLIPSE flow simulator - Schlumberger hand_ecl100 - Claus Aranha (caranha@cs.tsukuba.ac.jp) hand_ecldat - Tony Hallam ECL100Handler ( ECLBaseHandler ) This modules runs the ECLIPSE100 simulator in a \"sandbox\" directory, modifying the (prepared) parameter files with a set of test parameters. The results of the run are packaged for analysis by an optimizer. Source code in optuna_hm/handlers/_ecl_simh.py Python class ECL100Handler ( ECLBaseHandler ): \"\"\" This modules runs the ECLIPSE100 simulator in a \"sandbox\" directory, modifying the (prepared) parameter files with a set of test parameters. The results of the run are packaged for analysis by an optimizer. \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"ECL100Handler\" ) def __deepcopy__ ( self , memo ): # need to create a special copy function because the logger cannot be coppied # this means reinitialisation is necessary for each new simulator class args = [ self . __dict__ [ var ] for var in [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" , ] ] return ECL100Handler ( * args ) def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the ecl simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" modelfile = str ( self . model_name ) + \".DATA\" self . logger . info ( \"running model %s \" , os . path . join ( self . model_path , modelfile )) super () . launch ( self . model_name ) __init__ ( self , * args , ** kwargs ) special See ECLBaseHandler Notes: Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"ECL100Handler\" ) launch ( self , * cmdargs , ** cmdkwargs ) Launch the ecl simulator. Returns: Type Description bool True if run did not timeout. Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the ecl simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" modelfile = str ( self . model_name ) + \".DATA\" self . logger . info ( \"running model %s \" , os . path . join ( self . model_path , modelfile )) super () . launch ( self . model_name ) ECLBaseHandler Basic Class for interacting with ECLIPSE Simulation Program Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLBaseHandler : \"\"\"Basic Class for interacting with ECLIPSE Simulation Program\"\"\" def __init__ ( self , model_name , model_path , model_param , application_path = None , rerun_license_errors = True , license_error_sleep = 600 , time_out = 900 , silent = True , original = True , ): \"\"\" Args: application_path (str): Path to simulation executable model_name (str): The name of the ECLIPSE Model model_path (str): The path to the ECLIPSE Model model_param (pandas.DataFrame): The model parameters to update the parameter key should be in the index of the DF. time_out (int): Defaults to 900 seconds. Time before cancelling subprocess simulation run. silent (bool): Defaults to True, set to False to print STDOUT from subprocess call. original (bool): Is this the original model. If true changes to the model will not be permitted. \"\"\" self . model_name = model_name self . model_path = model_path self . popt = model_param self . logger = logging . getLogger ( \"ECLBaseHandler\" ) self . temp_dir = None self . original = original self . log_path = None self . id = \"00000000\" self . rerun_le = rerun_license_errors self . sleep_le = license_error_sleep self . silent = silent if application_path is not None : self . set_ecl ( application_path ) else : self . simexe = None self . time_out = time_out def set_ecl ( self , application_path ): \"\"\"Set the simulator application path\"\"\" self . simexe = application_path if not pathlib . Path ( self . simexe ) . exists (): raise ValueError ( f \"Cannot find simulator executable { self . simexe } \" ) def __deepcopy__ ( self , memo ): # create a copy but not logger copy_list = [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" ] obj_copy = object . __new__ ( type ( self )) for item in copy_list : obj_copy . __dict__ [ item ] = copy . deepcopy ( self . __dict__ [ item ], memo ) return obj_copy def update_ecl_prop ( self , prop ): \"\"\"Creates appropriate property values from random floats in range 0-1 Args: prop (dict): list or random floats in range (0-1) supplied by population update \"\"\" # normalise prop random floats (0-1) to property ranges # this replaces unnormalise if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) self . popt [ \"new_values\" ] = self . popt [ \"NAME\" ] . map ( prop ) for file_name , val in self . popt . groupby ( \"FILE\" ): filepath = os . path . join ( self . model_path , file_name ) updatefile = ECLDatHandler ( filepath ) # change this to write actual update values updatefile . update ( val [[ \"NAME\" , \"new_values\" ]] . set_index ( \"NAME\" )) self . popt = self . popt . drop ( columns = \"new_values\" ) def retrieve_ecl_prop ( self ): \"\"\"Retrieve objective parameter values Returns: (list): list of parameters corresponding to flags \"\"\" values = dict () flags = self . popt . index . values flags = [ flg . upper () for flg in flags ] for file_name , _ in self . popt . groupby ( \"FILE\" ): filepath = self . model_path / file_name with open ( filepath , \"r\" ) as fdat : lines = fdat . readlines () for line in lines : for flag in flags : if flag in line : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) values [ flag ] = float ( split [ ind ]) return [ values [ file_name ] for file_name in flags ] def set_model_path ( self , newpath ): \"\"\"Set the path to the model\"\"\" self . model_path = newpath def set_save_sim_logs ( self , path ): \"\"\"Set location to save model logs. For Eclipse these are the DBG files created by the run. Args: path (str): The path to save the logs to. If directory doesn't exist it will be created. \"\"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () self . log_path = path logging . info ( \"Simulation logs will be saved to: %s \" , str ( path )) def save_model ( self , path , prefix = None ): \"\"\"Save the whole model folder to a new path. Args: path (pathlike) prefix (str, optional) \"\"\" new_folder_name = \"_\" . join if prefix is not None : new_folder_name = f \" { prefix } _ { self . model_name } \" else : new_folder_name = self . model_name save_path = pathlib . Path ( path ) / new_folder_name if not save_path . exists (): save_path . mkdir ( parents = True , exist_ok = True ) for item in self . model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , save_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( save_path / item . name )) logging . info ( f \"Saved model successfully to { str ( save_path ) } \" ) def save_model_outputs ( self , path , files , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: files (list): List of files to save. outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () for file in files : file = pathlib . Path ( file ) name = self . id + \"_\" + file . name shutil . copy2 ( self . model_path / file , path / name ) def set_save_model_outputs ( self , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" if outputs is None : shutil . copytree ( self . model_path , prefix + \"_\" + self . id ) def create_working_model ( self , working_dir = None , working_cwd = None ): \"\"\"Create a working copy of this simulator so it can be modified without destroying the original. Args: working_dir (str): If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. working_cwd (str): If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. Returns: (EclBaseHandler): A copy of simulator with a new working directory. \"\"\" # create working directory and simulator class working_simulator = copy . deepcopy ( self ) if working_cwd is None : working_cwd = os . getcwd () if working_dir is None : temp_dir = tempfile . TemporaryDirectory ( prefix = f \"tempECL_ { self . model_name } \" , dir = working_cwd ) # store temp dir with model so we don't lose it when exiting this namespace working_simulator . temp_dir = temp_dir working_dir = temp_dir . name working_simulator . id = temp_dir . name [ - 8 :] else : temp_dir = working_dir working_simulator . id = working_dir working_path = pathlib . Path ( working_dir ) logging . info ( \"working directory is %s \" , working_path ) working_simulator . set_model_path ( working_path ) # get the original model path model_path = pathlib . Path ( self . model_path ) model_path_size = sum ( f . stat () . st_size for f in model_path . iterdir () if f . is_file () ) # copy files into working directory - THIS could be updated to give more info logging . info ( \"Copying model directory to %s \" , working_path ) for item in model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , working_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( working_path / item . name )) logging . info ( \"Coppied model successfully\" ) working_simulator . original = False # working_simulator.id = temp_dir.name[-8:] working_simulator . log_path = copy . deepcopy ( self . log_path ) copied = False while not copied : working_path_size = sum ( f . stat () . st_size for f in working_path . iterdir () if f . is_file () ) if working_path_size == model_path_size : copied = True time . sleep ( 5 ) # print(working_path.name, working_path_size, model_path_size, copied) return working_simulator def _launch_internal_loop ( self , cmd ): process = subprocess . Popen ( cmd , cwd = self . model_path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) count = 0 stdout = \"\" stderr = \"\" while True : proc_status = process . poll () if count > self . time_out : process . terminate () self . logger . warning ( \"Simulation Run stopped due to timeout\" ) return False , stdout , proc_status output = process . stdout . read () . decode () error = process . stderr . read () . decode () if self . log_path is not None : with open ( self . log_path / \"optuna-hm-stdout.txt\" , mode = \"a\" ) as f : f . write ( output ) with open ( self . log_path / \"optuna-hm-stderr.txt\" , mode = \"a\" ) as f : f . write ( error ) elif not self . silent : print ( output ) print ( error ) if output == \"\" and proc_status is not None : self . logger . info ( \"Model run finished with exit %s \" , proc_status ) return True , stdout , proc_status time . sleep ( 1 ) count = count + 1 def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launches an eclipse instance, run a simulation.\"\"\" if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) start_time = time . time () cmd = [ self . simexe ] for c in cmdargs : cmd . append ( c ) for cmdkw in cmdkwargs : cmd = cmd + [ f \"- { cmdkw } \" , str ( cmdkwargs [ cmdkw ])] print ( cmd ) while True : good_run , stdout , proc_status = self . _launch_internal_loop ( cmd ) if self . rerun_le : # there was a problem with the license server if \"LICENSE FAILURE\" in stdout : self . logger . warn ( \"License acquisition failure for eclipse.\" ) time . sleep ( self . sleep_le ) # if is int, count down number of valid times to retry if isinstance ( self . rerun_le , int ): self . rerun_le -= 1 # everything was ok else : break else : # don't check for license errors break # copy simulation terminal log and debug to save path if requested if self . log_path is not None : # copying debug file log = self . model_path / self . model_name log = log . with_suffix ( \".DBG\" ) if log . exists (): cp_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) cp_to = cp_to . with_suffix ( \".DBG\" ) shutil . copyfile ( log , cp_to ) self . logger . info ( \"Coppied simulation log file.\" ) else : self . logger . info ( \"Could not find log file.\" ) # writing out STDOUT write_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) write_to = write_to . with_suffix ( \".STDOUT\" ) with open ( write_to , \"w\" ) as stdout_log : stdout_log . write ( stdout ) self . logger . info ( \"Wrote simulation STDOUT\" ) # TODO - Save model ouputs if requested. end_time = time . time () - start_time if proc_status == 0 : self . logger . info ( \"simulation completed in %s seconds\" , str ( end_time )) return good_run def cleanup ( self , retries = 4 , sleep = 10 ): \"\"\"Cleanup the working directory correctly. Args: retries (int, optional): [description]. Defaults to 4. sleep (int, optional): [description]. Defaults to 10. \"\"\" if self . original : logging . info ( \"You cannot cleanup the original model.\" ) else : can_delete = False tries = 0 while not can_delete : time . sleep ( sleep ) tries = tries + 1 try : self . temp_dir . cleanup () can_delete = True except OSError : # can't delete just now if tries > retries : pass __init__ ( self , model_name , model_path , model_param , application_path = None , rerun_license_errors = True , license_error_sleep = 600 , time_out = 900 , silent = True , original = True ) special Parameters: Name Type Description Default application_path str Path to simulation executable None model_name str The name of the ECLIPSE Model required model_path str The path to the ECLIPSE Model required model_param pandas.DataFrame The model parameters to update the parameter key should be in the index of the DF. required time_out int Defaults to 900 seconds. Time before cancelling subprocess simulation run. 900 silent bool Defaults to True, set to False to print STDOUT from subprocess call. True original bool Is this the original model. If true changes to the model will not be permitted. True Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , model_name , model_path , model_param , application_path = None , rerun_license_errors = True , license_error_sleep = 600 , time_out = 900 , silent = True , original = True , ): \"\"\" Args: application_path (str): Path to simulation executable model_name (str): The name of the ECLIPSE Model model_path (str): The path to the ECLIPSE Model model_param (pandas.DataFrame): The model parameters to update the parameter key should be in the index of the DF. time_out (int): Defaults to 900 seconds. Time before cancelling subprocess simulation run. silent (bool): Defaults to True, set to False to print STDOUT from subprocess call. original (bool): Is this the original model. If true changes to the model will not be permitted. \"\"\" self . model_name = model_name self . model_path = model_path self . popt = model_param self . logger = logging . getLogger ( \"ECLBaseHandler\" ) self . temp_dir = None self . original = original self . log_path = None self . id = \"00000000\" self . rerun_le = rerun_license_errors self . sleep_le = license_error_sleep self . silent = silent if application_path is not None : self . set_ecl ( application_path ) else : self . simexe = None self . time_out = time_out cleanup ( self , retries = 4 , sleep = 10 ) Cleanup the working directory correctly. Parameters: Name Type Description Default retries int [description]. Defaults to 4. 4 sleep int [description]. Defaults to 10. 10 Source code in optuna_hm/handlers/_ecl_simh.py Python def cleanup ( self , retries = 4 , sleep = 10 ): \"\"\"Cleanup the working directory correctly. Args: retries (int, optional): [description]. Defaults to 4. sleep (int, optional): [description]. Defaults to 10. \"\"\" if self . original : logging . info ( \"You cannot cleanup the original model.\" ) else : can_delete = False tries = 0 while not can_delete : time . sleep ( sleep ) tries = tries + 1 try : self . temp_dir . cleanup () can_delete = True except OSError : # can't delete just now if tries > retries : pass create_working_model ( self , working_dir = None , working_cwd = None ) Create a working copy of this simulator so it can be modified without destroying the original. Parameters: Name Type Description Default working_dir str If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. None working_cwd str If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. None Returns: Type Description (EclBaseHandler) A copy of simulator with a new working directory. Source code in optuna_hm/handlers/_ecl_simh.py Python def create_working_model ( self , working_dir = None , working_cwd = None ): \"\"\"Create a working copy of this simulator so it can be modified without destroying the original. Args: working_dir (str): If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. working_cwd (str): If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. Returns: (EclBaseHandler): A copy of simulator with a new working directory. \"\"\" # create working directory and simulator class working_simulator = copy . deepcopy ( self ) if working_cwd is None : working_cwd = os . getcwd () if working_dir is None : temp_dir = tempfile . TemporaryDirectory ( prefix = f \"tempECL_ { self . model_name } \" , dir = working_cwd ) # store temp dir with model so we don't lose it when exiting this namespace working_simulator . temp_dir = temp_dir working_dir = temp_dir . name working_simulator . id = temp_dir . name [ - 8 :] else : temp_dir = working_dir working_simulator . id = working_dir working_path = pathlib . Path ( working_dir ) logging . info ( \"working directory is %s \" , working_path ) working_simulator . set_model_path ( working_path ) # get the original model path model_path = pathlib . Path ( self . model_path ) model_path_size = sum ( f . stat () . st_size for f in model_path . iterdir () if f . is_file () ) # copy files into working directory - THIS could be updated to give more info logging . info ( \"Copying model directory to %s \" , working_path ) for item in model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , working_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( working_path / item . name )) logging . info ( \"Coppied model successfully\" ) working_simulator . original = False # working_simulator.id = temp_dir.name[-8:] working_simulator . log_path = copy . deepcopy ( self . log_path ) copied = False while not copied : working_path_size = sum ( f . stat () . st_size for f in working_path . iterdir () if f . is_file () ) if working_path_size == model_path_size : copied = True time . sleep ( 5 ) # print(working_path.name, working_path_size, model_path_size, copied) return working_simulator launch ( self , * cmdargs , ** cmdkwargs ) Launches an eclipse instance, run a simulation. Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launches an eclipse instance, run a simulation.\"\"\" if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) start_time = time . time () cmd = [ self . simexe ] for c in cmdargs : cmd . append ( c ) for cmdkw in cmdkwargs : cmd = cmd + [ f \"- { cmdkw } \" , str ( cmdkwargs [ cmdkw ])] print ( cmd ) while True : good_run , stdout , proc_status = self . _launch_internal_loop ( cmd ) if self . rerun_le : # there was a problem with the license server if \"LICENSE FAILURE\" in stdout : self . logger . warn ( \"License acquisition failure for eclipse.\" ) time . sleep ( self . sleep_le ) # if is int, count down number of valid times to retry if isinstance ( self . rerun_le , int ): self . rerun_le -= 1 # everything was ok else : break else : # don't check for license errors break # copy simulation terminal log and debug to save path if requested if self . log_path is not None : # copying debug file log = self . model_path / self . model_name log = log . with_suffix ( \".DBG\" ) if log . exists (): cp_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) cp_to = cp_to . with_suffix ( \".DBG\" ) shutil . copyfile ( log , cp_to ) self . logger . info ( \"Coppied simulation log file.\" ) else : self . logger . info ( \"Could not find log file.\" ) # writing out STDOUT write_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) write_to = write_to . with_suffix ( \".STDOUT\" ) with open ( write_to , \"w\" ) as stdout_log : stdout_log . write ( stdout ) self . logger . info ( \"Wrote simulation STDOUT\" ) # TODO - Save model ouputs if requested. end_time = time . time () - start_time if proc_status == 0 : self . logger . info ( \"simulation completed in %s seconds\" , str ( end_time )) return good_run retrieve_ecl_prop ( self ) Retrieve objective parameter values Returns: Type Description (list) list of parameters corresponding to flags Source code in optuna_hm/handlers/_ecl_simh.py Python def retrieve_ecl_prop ( self ): \"\"\"Retrieve objective parameter values Returns: (list): list of parameters corresponding to flags \"\"\" values = dict () flags = self . popt . index . values flags = [ flg . upper () for flg in flags ] for file_name , _ in self . popt . groupby ( \"FILE\" ): filepath = self . model_path / file_name with open ( filepath , \"r\" ) as fdat : lines = fdat . readlines () for line in lines : for flag in flags : if flag in line : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) values [ flag ] = float ( split [ ind ]) return [ values [ file_name ] for file_name in flags ] save_model ( self , path , prefix = None ) Save the whole model folder to a new path. Source code in optuna_hm/handlers/_ecl_simh.py Python def save_model ( self , path , prefix = None ): \"\"\"Save the whole model folder to a new path. Args: path (pathlike) prefix (str, optional) \"\"\" new_folder_name = \"_\" . join if prefix is not None : new_folder_name = f \" { prefix } _ { self . model_name } \" else : new_folder_name = self . model_name save_path = pathlib . Path ( path ) / new_folder_name if not save_path . exists (): save_path . mkdir ( parents = True , exist_ok = True ) for item in self . model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , save_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( save_path / item . name )) logging . info ( f \"Saved model successfully to { str ( save_path ) } \" ) save_model_outputs ( self , path , files , outputs = None , prefix = None ) Specify model outputs to save for debugging or further review. Parameters: Name Type Description Default files list List of files to save. required outputs list List of files to save. None Source code in optuna_hm/handlers/_ecl_simh.py Python def save_model_outputs ( self , path , files , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: files (list): List of files to save. outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () for file in files : file = pathlib . Path ( file ) name = self . id + \"_\" + file . name shutil . copy2 ( self . model_path / file , path / name ) set_ecl ( self , application_path ) Set the simulator application path Source code in optuna_hm/handlers/_ecl_simh.py Python def set_ecl ( self , application_path ): \"\"\"Set the simulator application path\"\"\" self . simexe = application_path if not pathlib . Path ( self . simexe ) . exists (): raise ValueError ( f \"Cannot find simulator executable { self . simexe } \" ) set_model_path ( self , newpath ) Set the path to the model Source code in optuna_hm/handlers/_ecl_simh.py Python def set_model_path ( self , newpath ): \"\"\"Set the path to the model\"\"\" self . model_path = newpath set_save_model_outputs ( self , outputs = None , prefix = None ) Specify model outputs to save for debugging or further review. Parameters: Name Type Description Default outputs list List of files to save. None Source code in optuna_hm/handlers/_ecl_simh.py Python def set_save_model_outputs ( self , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" if outputs is None : shutil . copytree ( self . model_path , prefix + \"_\" + self . id ) set_save_sim_logs ( self , path ) Set location to save model logs. For Eclipse these are the DBG files created by the run. Parameters: Name Type Description Default path str The path to save the logs to. If directory doesn't exist it will be created. required Source code in optuna_hm/handlers/_ecl_simh.py Python def set_save_sim_logs ( self , path ): \"\"\"Set location to save model logs. For Eclipse these are the DBG files created by the run. Args: path (str): The path to save the logs to. If directory doesn't exist it will be created. \"\"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () self . log_path = path logging . info ( \"Simulation logs will be saved to: %s \" , str ( path )) update_ecl_prop ( self , prop ) Creates appropriate property values from random floats in range 0-1 Parameters: Name Type Description Default prop dict list or random floats in range (0-1) supplied by population update required Source code in optuna_hm/handlers/_ecl_simh.py Python def update_ecl_prop ( self , prop ): \"\"\"Creates appropriate property values from random floats in range 0-1 Args: prop (dict): list or random floats in range (0-1) supplied by population update \"\"\" # normalise prop random floats (0-1) to property ranges # this replaces unnormalise if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) self . popt [ \"new_values\" ] = self . popt [ \"NAME\" ] . map ( prop ) for file_name , val in self . popt . groupby ( \"FILE\" ): filepath = os . path . join ( self . model_path , file_name ) updatefile = ECLDatHandler ( filepath ) # change this to write actual update values updatefile . update ( val [[ \"NAME\" , \"new_values\" ]] . set_index ( \"NAME\" )) self . popt = self . popt . drop ( columns = \"new_values\" ) ECLDatHandler Class to update simulation properties based on a comment flag Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLDatHandler : \"\"\" Class to update simulation properties based on a comment flag \"\"\" def __init__ ( self , filename ): \"\"\"\"\"\" logger = logging . getLogger ( __name__ ) try : fdat = open ( filename , \"r\" ) fdat . close () self . filename = filename except FileNotFoundError as err : logger . error ( \"No such ECL DAT file: %s \" , filename ) raise type ( err ) def update ( self , new_values ): \"\"\"Replace objective parameter values with *new_values* in a ECL .DAT file using pre-defined *flags* Args: flags (list): List of parameter flags for new_values new_values (list): List of values to replace where flags Example: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. \"\"\" # check length of flgas and new values - should be the same # load entire file - it is difficult to edit in-place with open ( self . filename , \"r\" ) as fdat : lines = fdat . readlines () # perform edits for i , line in enumerate ( lines ): if line [: 2 ] == \"--\" : pass else : try : for flag in new_values . index : if re . search ( f \"\\s { flag } \\s\" , line ) is not None : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) split [ ind ] = \" {:.4E} \" . format ( new_values [ \"new_values\" ][ flag ]) line = \" \" . join ( split ) + \" \\n \" lines [ i ] = line except ValueError : print ( \"Error updating simulation file.\" ) print ( f \"Problem file: { self . filename } , Problem flag: { flag } \" ) print ( f \"Problem line: { line } \" ) raise SystemExit # write back file with open ( self . filename , \"w\" ) as fdat : fdat . writelines ( lines ) update ( self , new_values ) Replace objective parameter values with new_values in a ECL .DAT file using pre-defined flags Parameters: Name Type Description Default flags list List of parameter flags for new_values required new_values list List of values to replace where flags required Examples: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. Source code in optuna_hm/handlers/_ecl_simh.py Python def update ( self , new_values ): \"\"\"Replace objective parameter values with *new_values* in a ECL .DAT file using pre-defined *flags* Args: flags (list): List of parameter flags for new_values new_values (list): List of values to replace where flags Example: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. \"\"\" # check length of flgas and new values - should be the same # load entire file - it is difficult to edit in-place with open ( self . filename , \"r\" ) as fdat : lines = fdat . readlines () # perform edits for i , line in enumerate ( lines ): if line [: 2 ] == \"--\" : pass else : try : for flag in new_values . index : if re . search ( f \"\\s { flag } \\s\" , line ) is not None : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) split [ ind ] = \" {:.4E} \" . format ( new_values [ \"new_values\" ][ flag ]) line = \" \" . join ( split ) + \" \\n \" lines [ i ] = line except ValueError : print ( \"Error updating simulation file.\" ) print ( f \"Problem file: { self . filename } , Problem flag: { flag } \" ) print ( f \"Problem line: { line } \" ) raise SystemExit # write back file with open ( self . filename , \"w\" ) as fdat : fdat . writelines ( lines ) ECLFakeHandler ( ECLBaseHandler ) Create Fake RSM files to run tests without ECLIPSE Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLFakeHandler ( ECLBaseHandler ): \"\"\"Create Fake RSM files to run tests without ECLIPSE\"\"\" def __init__ ( self , target , reference , * args ): \"\"\"Constructor Args: target (str): column name from handler.popt to reference (pandas.DataFrame): Dataframe containing RSM results of true solution. *args: Arguments for ECLBaseHandler \"\"\" super () . __init__ ( * args ) self . logger = logging . getLogger ( \"ECLFakeHandler\" ) self . target = target self . reference = reference def __deepcopy__ ( self , memo ): # create a copy but not logger return ECLFakeHandler ( copy . deepcopy ( self . target , memo ), copy . deepcopy ( self . reference , memo ), copy . deepcopy ( self . model_name , memo ), copy . deepcopy ( self . model_path , memo ), copy . deepcopy ( self . popt , memo ), ) def clean_results ( self , modelpath , model ): \"\"\"Remove previous results\"\"\" result_file = os . path . join ( modelpath , model , \".RSM\" ) if os . path . isfile ( result_file ): os . remove ( result_file ) self . logger . info ( \"cleaning previous results\" ) def write_rsm ( self , filepath , rsm_well ): \"\"\"Write out a fake RSM file Args: filepath (str): The filepath for the RSM File rsm_well (pandas.DataFrame): \"\"\" with open ( filepath , \"w\" ) as rsmf : rsmf . write ( \" \\n \" ) rsmf . write ( \" \\t SUMMARY OF RUN GENERATED BY ECLFakeHandler \\n \" ) header = [ \" \\t \" ] for col in rsm_well . columns : header += [ col , \" \\t\\t\\t \" ] header += [ \" \\n \" ] * 4 rsmf . write ( \"\" . join ( header )) ncol = len ( rsm_well . columns ) data_template = \" \\t \" + \" {:>8g} \\t\\t \" * ncol for _ , val in rsm_well . iterrows (): rsmf . write ( data_template . format ( * val ) + \" \\n \" ) def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the simulator\"\"\" param = numpy . r_ [ self . retrieve_ecl_prop ()] misfit = ( numpy . sum ( numpy . sqrt ( numpy . square ( param / self . popt [ self . target ] . values - 1 )) ) / param . size ) scalar = 1 + misfit rsm_well = pd . DataFrame () rsm_well [ \"TIME\" ] = self . reference [ \"TIME\" ] for col in self . reference . drop ( columns = \"TIME\" ) . columns : rsm_well [ col + \"H\" ] = self . reference [ col ] rsm_well [ col ] = self . reference [ col ] / scalar filepath = os . path . join ( self . model_path , self . model_name + \".RSM\" ) self . write_rsm ( filepath , rsm_well ) __init__ ( self , target , reference , * args ) special Constructor Parameters: Name Type Description Default target str column name from handler.popt to required reference pandas.DataFrame Dataframe containing RSM results of true solution. required *args Arguments for ECLBaseHandler () Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , target , reference , * args ): \"\"\"Constructor Args: target (str): column name from handler.popt to reference (pandas.DataFrame): Dataframe containing RSM results of true solution. *args: Arguments for ECLBaseHandler \"\"\" super () . __init__ ( * args ) self . logger = logging . getLogger ( \"ECLFakeHandler\" ) self . target = target self . reference = reference clean_results ( self , modelpath , model ) Remove previous results Source code in optuna_hm/handlers/_ecl_simh.py Python def clean_results ( self , modelpath , model ): \"\"\"Remove previous results\"\"\" result_file = os . path . join ( modelpath , model , \".RSM\" ) if os . path . isfile ( result_file ): os . remove ( result_file ) self . logger . info ( \"cleaning previous results\" ) launch ( self , * cmdargs , ** cmdkwargs ) Launch the simulator Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the simulator\"\"\" param = numpy . r_ [ self . retrieve_ecl_prop ()] misfit = ( numpy . sum ( numpy . sqrt ( numpy . square ( param / self . popt [ self . target ] . values - 1 )) ) / param . size ) scalar = 1 + misfit rsm_well = pd . DataFrame () rsm_well [ \"TIME\" ] = self . reference [ \"TIME\" ] for col in self . reference . drop ( columns = \"TIME\" ) . columns : rsm_well [ col + \"H\" ] = self . reference [ col ] rsm_well [ col ] = self . reference [ col ] / scalar filepath = os . path . join ( self . model_path , self . model_name + \".RSM\" ) self . write_rsm ( filepath , rsm_well ) write_rsm ( self , filepath , rsm_well ) Write out a fake RSM file Parameters: Name Type Description Default filepath str The filepath for the RSM File required rsm_well pandas.DataFrame required Source code in optuna_hm/handlers/_ecl_simh.py Python def write_rsm ( self , filepath , rsm_well ): \"\"\"Write out a fake RSM file Args: filepath (str): The filepath for the RSM File rsm_well (pandas.DataFrame): \"\"\" with open ( filepath , \"w\" ) as rsmf : rsmf . write ( \" \\n \" ) rsmf . write ( \" \\t SUMMARY OF RUN GENERATED BY ECLFakeHandler \\n \" ) header = [ \" \\t \" ] for col in rsm_well . columns : header += [ col , \" \\t\\t\\t \" ] header += [ \" \\n \" ] * 4 rsmf . write ( \"\" . join ( header )) ncol = len ( rsm_well . columns ) data_template = \" \\t \" + \" {:>8g} \\t\\t \" * ncol for _ , val in rsm_well . iterrows (): rsmf . write ( data_template . format ( * val ) + \" \\n \" ) A compilation of common evaluation functions used for eclipse optimisation projects. Author Tony Hallam 2020 Error ( Exception ) Base class for other exceptions Source code in optuna_hm/handlers/_ecl_eval.py Python class Error ( Exception ): \"\"\"Base class for other exceptions\"\"\" def __init__ ( self , message = None ): self . message = message logger . error ( message ) super () . __init__ () SimulationRunError ( Error ) Raised when the input value is too small Source code in optuna_hm/handlers/_ecl_eval.py Python class SimulationRunError ( Error ): \"\"\"Raised when the input value is too small\"\"\" pass evaluate_eclipse_model ( simulator , results_dir = None , save_model_run = False , save_summary_results = False , save_properties_results = False , save_model_files = None , sleep = 5 , working_cwd = None , run_sim = True , update_prop = True , summary_results_kwargs = None , properties_results_kwargs = None , ** kwargs ) Evaluate an eclipse model. Parameters: Name Type Description Default simulator mophim.eclipse.EclBaseHandler The initialised simulator class object which can be derived from EclBaseHandler required results_dir str, pathlib.Path A directory, the directory must exist. None Source code in optuna_hm/handlers/_ecl_eval.py Python def evaluate_eclipse_model ( simulator , results_dir = None , save_model_run = False , save_summary_results = False , save_properties_results = False , save_model_files = None , sleep = 5 , working_cwd = None , run_sim = True , update_prop = True , summary_results_kwargs = None , properties_results_kwargs = None , ** kwargs , ): \"\"\"Evaluate an eclipse model. Args: simulator (mophim.eclipse.EclBaseHandler): The initialised simulator class object which can be derived from EclBaseHandler results_dir (str, pathlib.Path): A directory, the directory must exist. \"\"\" if not isinstance ( simulator , ECLBaseHandler ): raise ValueError ( f \"simulator argument must be of class { type ( ECLBaseHandler ) } , got { type ( simulator ) } \" ) if results_dir is None : results_dir = pathlib . Path ( \".\" ) . absolute () else : results_dir = pathlib . Path ( results_dir ) . absolute () if not results_dir . is_dir (): raise ValueError ( f \"results_dir must be directory and exist { results_dir } \" ) results = kwargs [ \"results\" ] optimiser = results [ \"sampler\" ] evaluation = results [ \"trial_number\" ] # create working sim logger . info ( f \"Starting { optimiser } trial number { evaluation } \" ) working_simulator = simulator . create_working_model ( working_cwd = working_cwd ) time . sleep ( sleep ) if update_prop : working_simulator . update_ecl_prop ( results [ \"p\" ]) if run_sim : working_simulator . launch () # Wait for simulation finalisation and cleanup time . sleep ( sleep ) # Save model files if requested if save_model_files is not None : model_files_save_path = results_dir / \"model_files\" try : model_files_save_path . mkdir () except FileExistsError : pass working_simulator . save_model_outputs ( model_files_save_path , save_model_files ) model_path_name = ( pathlib . Path ( working_simulator . temp_dir . name ) / working_simulator . model_name ) # Save whole model run if save_model_run : working_simulator . save_model ( results_dir / \"ecl_models\" , prefix = f \" { int ( evaluation ) : 05d } \" ) # read in simulation summary data try : summary_results = { \"well\" : None , \"failed\" : False } if summary_results_kwargs is not None : summary_results = _get_ecl_sumspec_results ( model_path_name , ** summary_results_kwargs ) else : summary_results = _get_ecl_sumspec_results ( model_path_name ) except SimulationOutputError : # simulation failed. logger . warning ( \"Results file not found %s \" , str ( model_path_name )) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} except KeyError as err : # simulation failed. logger . warning ( \"Results keys missing from summary %s \" , err ) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} except SimulationRunError as err : # simulation raised errors - probably failed logger . warning ( \"Simulation run failed. %s \" , err ) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} if summary_results [ \"well\" ] is not None and save_summary_results : name = \" {} _ {:04d} _summary.hdf5\" . format ( optimiser , evaluation ) summary_results_path = results_dir / \"summary\" try : summary_results_path . mkdir () except FileExistsError : pass summary_results_path_name = summary_results_path / name summary_results [ \"well\" ] . to_hdf ( summary_results_path_name , \"name\" , mode = \"w\" , format = \"table\" ) del summary_results [ \"well\" ] summary_results [ \"well\" ] = str ( summary_results_path_name ) logger . info ( f \"Summary results hdf5 output { summary_results_path / name } \" ) # read in simulation property data try : eclsim = None if properties_results_kwargs is not None : eclsim = _get_ecl_properties_sub ( model_path_name , ** properties_results_kwargs ) # print(\"eclsim_ret\", eclsim) # except IOError as err: # logger.info(f\"Problems reading property results evaluation {evaluation}\") # print(err) # property_results = {\"eclsim\": eclsim, \"failed\": True} except ( IOError , OSError , SimulationOutputError ) as err : logger . warning ( f \"Problems reading property results evaluation { evaluation } \" ) property_results = { \"eclsim\" : eclsim , \"failed\" : True } except KeyError as err : logger . warning ( f \"Keys not found in results file evaluation { evaluation } \" ) property_results = { \"eclsim\" : eclsim , \"failed\" : True } else : property_results = { \"eclsim\" : eclsim , \"failed\" : False } if property_results [ \"eclsim\" ] is not None and save_properties_results : name = \" {} _ {:04d} _eclsim\" . format ( optimiser , evaluation ) property_results_path = results_dir / \"eclsim\" try : property_results_path . mkdir () except FileExistsError : pass eclsim_save_path_name = property_results_path / name property_results [ \"eclsim\" ] . save ( eclsim_save_path_name ) del property_results [ \"eclsim\" ] property_results [ \"eclsim\" ] = str ( eclsim_save_path_name ) logger . info ( f \"Summary results eclsim output { property_results_path / name } \" ) if summary_results [ \"failed\" ] is True or property_results [ \"failed\" ] is True : failed = True else : failed = False results . update ( summary_results ) results . update ( property_results ) results [ \"failed\" ] = failed results . update ({ \"model_id\" : working_simulator . id }) working_simulator . cleanup ( sleep = sleep ) logger . info ( f \"Eclipse run finished and data loaded, trial number { results [ 'trial_number' ] } \" ) return results evaluate_well_prod ( log_pairs , date_range = None , method = 'kge' , data_key = 'well' , null_fitness = None , ** kwargs ) Calculated the misfit between all log pairs using method and returns a results dictionary labelled by the first log in the pair. Parameters: Name Type Description Default log_pairs list[(, ), ] A list of length 2 tuples which match labels in the input dataframe passed by the mopphim_kwargs 'results' required date_range tuple(str, str Filter the dataframe based upon a date range. The index of the input dataframe will need to be of type datetime. If the str is empty e.g. \"\" then the filter will be open in that direction. E.g. ('2020-2-02', '') or ('2020-2-02'. '2020-4-04') None columns list List of DataFrame values to perform metric on. required Source code in optuna_hm/handlers/_ecl_eval.py Python def evaluate_well_prod ( log_pairs , date_range = None , method = \"kge\" , data_key = \"well\" , null_fitness = None , ** kwargs , ): \"\"\"Calculated the misfit between all log pairs using method and returns a results dictionary labelled by the first log in the pair. Args: log_pairs (list[(, ), ]): A list of length 2 tuples which match labels in the input dataframe passed by the mopphim_kwargs['results'][data_key] date_range (tuple(str, str)): Filter the dataframe based upon a date range. The index of the input dataframe will need to be of type datetime. If the str is empty e.g. \"\" then the filter will be open in that direction. E.g. ('2020-2-02', '') or ('2020-2-02'. '2020-4-04') columns (list): List of DataFrame values to perform metric on. \"\"\" # set output metrics to null_fitness metrics = { p1 : null_fitness for p1 , _ in log_pairs } if kwargs [ \"results\" ][ \"failed\" ] and kwargs [ \"results\" ][ \"well\" ] is None : return metrics results = kwargs [ \"results\" ] data_path_name = kwargs [ \"results\" ][ data_key ] data = pd . read_hdf ( data_path_name ) available_logs = data . columns output_metrics = dict () if data is None : logger . debug ( \"Missing well results from ECL run. Skipping well evaluation.\" ) return metrics # check log_pairs for log in np . array ( log_pairs ) . flatten (): if not log in available_logs : raise ValueError ( f \"log { log } was not in results\" ) clean_data = data . dropna () if date_range is not None : clean_data = clean_data [ date_range [ 0 ] : date_range [ 1 ]] if clean_data . shape [ 0 ] == 0 : logger . debug ( \"data is empty, check date_range/simulation\" ) return metrics # NOT SURE IF THIS IS STILL NEEDED # no_data = [c for c in plogs if clean_data[c[0]].sum() == 0] # plogs = [c for c in plogs if clean_data[c[0]].sum() > 0] # for c in no_data: # metrics[c[0]] = np.nan if method == \"kge\" : # kling-gupta efficieny for p1 , p2 in log_pairs : output_metrics [ p1 ] = 1 - kge ( clean_data [ p1 ] . values , clean_data [ p2 ] . values , maxfitness = null_fitness , # incase method fails ) # elif method == \"mse\": # scaler = StandardScaler() # data_norm = clean_data.copy() # data_norm.loc[:, :] = scaler.fit_transform(clean_data.values) # for col in plogs: # metrics[col[0]] = mean_squared_error( # data_norm[col[0]].values, data_norm[col[1]].values # ) else : raise ValueError ( f \"Unknown method { method } \" ) logger ( f \"Completed well evaluation, trial_number { results [ 'trial_number' ] } \" ) return output_metrics optuna_hm.handlers.flow Flow is an alternative open-source simulator to Eclipse. It uses similar files as input and output but has a slightly different call signature. This module modifies the Eclipse methods to suit flow where needed. Evaluation and metrics functions from Eclipse can be used with Flow. Handler module for external file and process handling of ECLIPSE flow simulator - Schlumberger hand_ecl100 - Claus Aranha (caranha@cs.tsukuba.ac.jp) hand_ecldat - Tony Hallam FlowHandler ( ECLBaseHandler ) modifies ECLBaseHandler to deal with flow. Source code in optuna_hm/handlers/flow.py Python class FlowHandler ( ECLBaseHandler ): \"\"\" modifies ECLBaseHandler to deal with flow. \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"FlowHandler\" ) def __deepcopy__ ( self , memo ): # need to create a special copy function because the logger cannot be coppied # this means reinitialisation is necessary for each new simulator class args = [ self . __dict__ [ var ] for var in [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" , ] ] return FlowHandler ( * args ) def clean_results (): # FIXME: delete results file from self.model_path # beginning with self.model_name pass def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the flow simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" model_arg = \"--parameter-file=\" + str ( self . model_name ) + \".PARAM\" self . logger . info ( \"running model at %s \" , self . model_path ) return super () . launch ( model_arg ) __init__ ( self , * args , ** kwargs ) special See ECLBaseHandler Notes: Source code in optuna_hm/handlers/flow.py Python def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"FlowHandler\" ) launch ( self , * cmdargs , ** cmdkwargs ) Launch the flow simulator. Returns: Type Description bool True if run did not timeout. Source code in optuna_hm/handlers/flow.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the flow simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" model_arg = \"--parameter-file=\" + str ( self . model_name ) + \".PARAM\" self . logger . info ( \"running model at %s \" , self . model_path ) return super () . launch ( model_arg )","title":"Handlers"},{"location":"ophm/api_handlers.html#handlers","text":"Python from optuna_hm import handlers","title":"Handlers"},{"location":"ophm/api_handlers.html#optuna_hmhandlersecl","text":"Handlers for interfacing with and evaluating Eclipse (Schlumberger) Reservoir Simulation models. Eclipse is a proprietary simulation software which has a command line interface. Handler module for external file and process handling of ECLIPSE flow simulator - Schlumberger hand_ecl100 - Claus Aranha (caranha@cs.tsukuba.ac.jp) hand_ecldat - Tony Hallam","title":"optuna_hm.handlers.ecl"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECL100Handler","text":"This modules runs the ECLIPSE100 simulator in a \"sandbox\" directory, modifying the (prepared) parameter files with a set of test parameters. The results of the run are packaged for analysis by an optimizer. Source code in optuna_hm/handlers/_ecl_simh.py Python class ECL100Handler ( ECLBaseHandler ): \"\"\" This modules runs the ECLIPSE100 simulator in a \"sandbox\" directory, modifying the (prepared) parameter files with a set of test parameters. The results of the run are packaged for analysis by an optimizer. \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"ECL100Handler\" ) def __deepcopy__ ( self , memo ): # need to create a special copy function because the logger cannot be coppied # this means reinitialisation is necessary for each new simulator class args = [ self . __dict__ [ var ] for var in [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" , ] ] return ECL100Handler ( * args ) def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the ecl simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" modelfile = str ( self . model_name ) + \".DATA\" self . logger . info ( \"running model %s \" , os . path . join ( self . model_path , modelfile )) super () . launch ( self . model_name )","title":"ECL100Handler"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECL100Handler.__init__","text":"See ECLBaseHandler Notes: Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"ECL100Handler\" )","title":"__init__()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECL100Handler.launch","text":"Launch the ecl simulator. Returns: Type Description bool True if run did not timeout. Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the ecl simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" modelfile = str ( self . model_name ) + \".DATA\" self . logger . info ( \"running model %s \" , os . path . join ( self . model_path , modelfile )) super () . launch ( self . model_name )","title":"launch()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler","text":"Basic Class for interacting with ECLIPSE Simulation Program Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLBaseHandler : \"\"\"Basic Class for interacting with ECLIPSE Simulation Program\"\"\" def __init__ ( self , model_name , model_path , model_param , application_path = None , rerun_license_errors = True , license_error_sleep = 600 , time_out = 900 , silent = True , original = True , ): \"\"\" Args: application_path (str): Path to simulation executable model_name (str): The name of the ECLIPSE Model model_path (str): The path to the ECLIPSE Model model_param (pandas.DataFrame): The model parameters to update the parameter key should be in the index of the DF. time_out (int): Defaults to 900 seconds. Time before cancelling subprocess simulation run. silent (bool): Defaults to True, set to False to print STDOUT from subprocess call. original (bool): Is this the original model. If true changes to the model will not be permitted. \"\"\" self . model_name = model_name self . model_path = model_path self . popt = model_param self . logger = logging . getLogger ( \"ECLBaseHandler\" ) self . temp_dir = None self . original = original self . log_path = None self . id = \"00000000\" self . rerun_le = rerun_license_errors self . sleep_le = license_error_sleep self . silent = silent if application_path is not None : self . set_ecl ( application_path ) else : self . simexe = None self . time_out = time_out def set_ecl ( self , application_path ): \"\"\"Set the simulator application path\"\"\" self . simexe = application_path if not pathlib . Path ( self . simexe ) . exists (): raise ValueError ( f \"Cannot find simulator executable { self . simexe } \" ) def __deepcopy__ ( self , memo ): # create a copy but not logger copy_list = [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" ] obj_copy = object . __new__ ( type ( self )) for item in copy_list : obj_copy . __dict__ [ item ] = copy . deepcopy ( self . __dict__ [ item ], memo ) return obj_copy def update_ecl_prop ( self , prop ): \"\"\"Creates appropriate property values from random floats in range 0-1 Args: prop (dict): list or random floats in range (0-1) supplied by population update \"\"\" # normalise prop random floats (0-1) to property ranges # this replaces unnormalise if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) self . popt [ \"new_values\" ] = self . popt [ \"NAME\" ] . map ( prop ) for file_name , val in self . popt . groupby ( \"FILE\" ): filepath = os . path . join ( self . model_path , file_name ) updatefile = ECLDatHandler ( filepath ) # change this to write actual update values updatefile . update ( val [[ \"NAME\" , \"new_values\" ]] . set_index ( \"NAME\" )) self . popt = self . popt . drop ( columns = \"new_values\" ) def retrieve_ecl_prop ( self ): \"\"\"Retrieve objective parameter values Returns: (list): list of parameters corresponding to flags \"\"\" values = dict () flags = self . popt . index . values flags = [ flg . upper () for flg in flags ] for file_name , _ in self . popt . groupby ( \"FILE\" ): filepath = self . model_path / file_name with open ( filepath , \"r\" ) as fdat : lines = fdat . readlines () for line in lines : for flag in flags : if flag in line : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) values [ flag ] = float ( split [ ind ]) return [ values [ file_name ] for file_name in flags ] def set_model_path ( self , newpath ): \"\"\"Set the path to the model\"\"\" self . model_path = newpath def set_save_sim_logs ( self , path ): \"\"\"Set location to save model logs. For Eclipse these are the DBG files created by the run. Args: path (str): The path to save the logs to. If directory doesn't exist it will be created. \"\"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () self . log_path = path logging . info ( \"Simulation logs will be saved to: %s \" , str ( path )) def save_model ( self , path , prefix = None ): \"\"\"Save the whole model folder to a new path. Args: path (pathlike) prefix (str, optional) \"\"\" new_folder_name = \"_\" . join if prefix is not None : new_folder_name = f \" { prefix } _ { self . model_name } \" else : new_folder_name = self . model_name save_path = pathlib . Path ( path ) / new_folder_name if not save_path . exists (): save_path . mkdir ( parents = True , exist_ok = True ) for item in self . model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , save_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( save_path / item . name )) logging . info ( f \"Saved model successfully to { str ( save_path ) } \" ) def save_model_outputs ( self , path , files , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: files (list): List of files to save. outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () for file in files : file = pathlib . Path ( file ) name = self . id + \"_\" + file . name shutil . copy2 ( self . model_path / file , path / name ) def set_save_model_outputs ( self , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" if outputs is None : shutil . copytree ( self . model_path , prefix + \"_\" + self . id ) def create_working_model ( self , working_dir = None , working_cwd = None ): \"\"\"Create a working copy of this simulator so it can be modified without destroying the original. Args: working_dir (str): If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. working_cwd (str): If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. Returns: (EclBaseHandler): A copy of simulator with a new working directory. \"\"\" # create working directory and simulator class working_simulator = copy . deepcopy ( self ) if working_cwd is None : working_cwd = os . getcwd () if working_dir is None : temp_dir = tempfile . TemporaryDirectory ( prefix = f \"tempECL_ { self . model_name } \" , dir = working_cwd ) # store temp dir with model so we don't lose it when exiting this namespace working_simulator . temp_dir = temp_dir working_dir = temp_dir . name working_simulator . id = temp_dir . name [ - 8 :] else : temp_dir = working_dir working_simulator . id = working_dir working_path = pathlib . Path ( working_dir ) logging . info ( \"working directory is %s \" , working_path ) working_simulator . set_model_path ( working_path ) # get the original model path model_path = pathlib . Path ( self . model_path ) model_path_size = sum ( f . stat () . st_size for f in model_path . iterdir () if f . is_file () ) # copy files into working directory - THIS could be updated to give more info logging . info ( \"Copying model directory to %s \" , working_path ) for item in model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , working_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( working_path / item . name )) logging . info ( \"Coppied model successfully\" ) working_simulator . original = False # working_simulator.id = temp_dir.name[-8:] working_simulator . log_path = copy . deepcopy ( self . log_path ) copied = False while not copied : working_path_size = sum ( f . stat () . st_size for f in working_path . iterdir () if f . is_file () ) if working_path_size == model_path_size : copied = True time . sleep ( 5 ) # print(working_path.name, working_path_size, model_path_size, copied) return working_simulator def _launch_internal_loop ( self , cmd ): process = subprocess . Popen ( cmd , cwd = self . model_path , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) count = 0 stdout = \"\" stderr = \"\" while True : proc_status = process . poll () if count > self . time_out : process . terminate () self . logger . warning ( \"Simulation Run stopped due to timeout\" ) return False , stdout , proc_status output = process . stdout . read () . decode () error = process . stderr . read () . decode () if self . log_path is not None : with open ( self . log_path / \"optuna-hm-stdout.txt\" , mode = \"a\" ) as f : f . write ( output ) with open ( self . log_path / \"optuna-hm-stderr.txt\" , mode = \"a\" ) as f : f . write ( error ) elif not self . silent : print ( output ) print ( error ) if output == \"\" and proc_status is not None : self . logger . info ( \"Model run finished with exit %s \" , proc_status ) return True , stdout , proc_status time . sleep ( 1 ) count = count + 1 def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launches an eclipse instance, run a simulation.\"\"\" if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) start_time = time . time () cmd = [ self . simexe ] for c in cmdargs : cmd . append ( c ) for cmdkw in cmdkwargs : cmd = cmd + [ f \"- { cmdkw } \" , str ( cmdkwargs [ cmdkw ])] print ( cmd ) while True : good_run , stdout , proc_status = self . _launch_internal_loop ( cmd ) if self . rerun_le : # there was a problem with the license server if \"LICENSE FAILURE\" in stdout : self . logger . warn ( \"License acquisition failure for eclipse.\" ) time . sleep ( self . sleep_le ) # if is int, count down number of valid times to retry if isinstance ( self . rerun_le , int ): self . rerun_le -= 1 # everything was ok else : break else : # don't check for license errors break # copy simulation terminal log and debug to save path if requested if self . log_path is not None : # copying debug file log = self . model_path / self . model_name log = log . with_suffix ( \".DBG\" ) if log . exists (): cp_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) cp_to = cp_to . with_suffix ( \".DBG\" ) shutil . copyfile ( log , cp_to ) self . logger . info ( \"Coppied simulation log file.\" ) else : self . logger . info ( \"Could not find log file.\" ) # writing out STDOUT write_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) write_to = write_to . with_suffix ( \".STDOUT\" ) with open ( write_to , \"w\" ) as stdout_log : stdout_log . write ( stdout ) self . logger . info ( \"Wrote simulation STDOUT\" ) # TODO - Save model ouputs if requested. end_time = time . time () - start_time if proc_status == 0 : self . logger . info ( \"simulation completed in %s seconds\" , str ( end_time )) return good_run def cleanup ( self , retries = 4 , sleep = 10 ): \"\"\"Cleanup the working directory correctly. Args: retries (int, optional): [description]. Defaults to 4. sleep (int, optional): [description]. Defaults to 10. \"\"\" if self . original : logging . info ( \"You cannot cleanup the original model.\" ) else : can_delete = False tries = 0 while not can_delete : time . sleep ( sleep ) tries = tries + 1 try : self . temp_dir . cleanup () can_delete = True except OSError : # can't delete just now if tries > retries : pass","title":"ECLBaseHandler"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.__init__","text":"Parameters: Name Type Description Default application_path str Path to simulation executable None model_name str The name of the ECLIPSE Model required model_path str The path to the ECLIPSE Model required model_param pandas.DataFrame The model parameters to update the parameter key should be in the index of the DF. required time_out int Defaults to 900 seconds. Time before cancelling subprocess simulation run. 900 silent bool Defaults to True, set to False to print STDOUT from subprocess call. True original bool Is this the original model. If true changes to the model will not be permitted. True Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , model_name , model_path , model_param , application_path = None , rerun_license_errors = True , license_error_sleep = 600 , time_out = 900 , silent = True , original = True , ): \"\"\" Args: application_path (str): Path to simulation executable model_name (str): The name of the ECLIPSE Model model_path (str): The path to the ECLIPSE Model model_param (pandas.DataFrame): The model parameters to update the parameter key should be in the index of the DF. time_out (int): Defaults to 900 seconds. Time before cancelling subprocess simulation run. silent (bool): Defaults to True, set to False to print STDOUT from subprocess call. original (bool): Is this the original model. If true changes to the model will not be permitted. \"\"\" self . model_name = model_name self . model_path = model_path self . popt = model_param self . logger = logging . getLogger ( \"ECLBaseHandler\" ) self . temp_dir = None self . original = original self . log_path = None self . id = \"00000000\" self . rerun_le = rerun_license_errors self . sleep_le = license_error_sleep self . silent = silent if application_path is not None : self . set_ecl ( application_path ) else : self . simexe = None self . time_out = time_out","title":"__init__()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.cleanup","text":"Cleanup the working directory correctly. Parameters: Name Type Description Default retries int [description]. Defaults to 4. 4 sleep int [description]. Defaults to 10. 10 Source code in optuna_hm/handlers/_ecl_simh.py Python def cleanup ( self , retries = 4 , sleep = 10 ): \"\"\"Cleanup the working directory correctly. Args: retries (int, optional): [description]. Defaults to 4. sleep (int, optional): [description]. Defaults to 10. \"\"\" if self . original : logging . info ( \"You cannot cleanup the original model.\" ) else : can_delete = False tries = 0 while not can_delete : time . sleep ( sleep ) tries = tries + 1 try : self . temp_dir . cleanup () can_delete = True except OSError : # can't delete just now if tries > retries : pass","title":"cleanup()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.create_working_model","text":"Create a working copy of this simulator so it can be modified without destroying the original. Parameters: Name Type Description Default working_dir str If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. None working_cwd str If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. None Returns: Type Description (EclBaseHandler) A copy of simulator with a new working directory. Source code in optuna_hm/handlers/_ecl_simh.py Python def create_working_model ( self , working_dir = None , working_cwd = None ): \"\"\"Create a working copy of this simulator so it can be modified without destroying the original. Args: working_dir (str): If None, a temporary directory in the working_cwd will be created. Otherwise, specify a path to a preferred working directory. Defaults to None. working_cwd (str): If None, use the current working director. Otherwise will create temporary space in working_cwd. Defaults to None. This can be useful when using distributed processing and you want models to be evaluated on remote storage. Returns: (EclBaseHandler): A copy of simulator with a new working directory. \"\"\" # create working directory and simulator class working_simulator = copy . deepcopy ( self ) if working_cwd is None : working_cwd = os . getcwd () if working_dir is None : temp_dir = tempfile . TemporaryDirectory ( prefix = f \"tempECL_ { self . model_name } \" , dir = working_cwd ) # store temp dir with model so we don't lose it when exiting this namespace working_simulator . temp_dir = temp_dir working_dir = temp_dir . name working_simulator . id = temp_dir . name [ - 8 :] else : temp_dir = working_dir working_simulator . id = working_dir working_path = pathlib . Path ( working_dir ) logging . info ( \"working directory is %s \" , working_path ) working_simulator . set_model_path ( working_path ) # get the original model path model_path = pathlib . Path ( self . model_path ) model_path_size = sum ( f . stat () . st_size for f in model_path . iterdir () if f . is_file () ) # copy files into working directory - THIS could be updated to give more info logging . info ( \"Copying model directory to %s \" , working_path ) for item in model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , working_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( working_path / item . name )) logging . info ( \"Coppied model successfully\" ) working_simulator . original = False # working_simulator.id = temp_dir.name[-8:] working_simulator . log_path = copy . deepcopy ( self . log_path ) copied = False while not copied : working_path_size = sum ( f . stat () . st_size for f in working_path . iterdir () if f . is_file () ) if working_path_size == model_path_size : copied = True time . sleep ( 5 ) # print(working_path.name, working_path_size, model_path_size, copied) return working_simulator","title":"create_working_model()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.launch","text":"Launches an eclipse instance, run a simulation. Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launches an eclipse instance, run a simulation.\"\"\" if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) start_time = time . time () cmd = [ self . simexe ] for c in cmdargs : cmd . append ( c ) for cmdkw in cmdkwargs : cmd = cmd + [ f \"- { cmdkw } \" , str ( cmdkwargs [ cmdkw ])] print ( cmd ) while True : good_run , stdout , proc_status = self . _launch_internal_loop ( cmd ) if self . rerun_le : # there was a problem with the license server if \"LICENSE FAILURE\" in stdout : self . logger . warn ( \"License acquisition failure for eclipse.\" ) time . sleep ( self . sleep_le ) # if is int, count down number of valid times to retry if isinstance ( self . rerun_le , int ): self . rerun_le -= 1 # everything was ok else : break else : # don't check for license errors break # copy simulation terminal log and debug to save path if requested if self . log_path is not None : # copying debug file log = self . model_path / self . model_name log = log . with_suffix ( \".DBG\" ) if log . exists (): cp_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) cp_to = cp_to . with_suffix ( \".DBG\" ) shutil . copyfile ( log , cp_to ) self . logger . info ( \"Coppied simulation log file.\" ) else : self . logger . info ( \"Could not find log file.\" ) # writing out STDOUT write_to = self . log_path / \"_\" . join ([ log . stem , self . id ]) write_to = write_to . with_suffix ( \".STDOUT\" ) with open ( write_to , \"w\" ) as stdout_log : stdout_log . write ( stdout ) self . logger . info ( \"Wrote simulation STDOUT\" ) # TODO - Save model ouputs if requested. end_time = time . time () - start_time if proc_status == 0 : self . logger . info ( \"simulation completed in %s seconds\" , str ( end_time )) return good_run","title":"launch()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.retrieve_ecl_prop","text":"Retrieve objective parameter values Returns: Type Description (list) list of parameters corresponding to flags Source code in optuna_hm/handlers/_ecl_simh.py Python def retrieve_ecl_prop ( self ): \"\"\"Retrieve objective parameter values Returns: (list): list of parameters corresponding to flags \"\"\" values = dict () flags = self . popt . index . values flags = [ flg . upper () for flg in flags ] for file_name , _ in self . popt . groupby ( \"FILE\" ): filepath = self . model_path / file_name with open ( filepath , \"r\" ) as fdat : lines = fdat . readlines () for line in lines : for flag in flags : if flag in line : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) values [ flag ] = float ( split [ ind ]) return [ values [ file_name ] for file_name in flags ]","title":"retrieve_ecl_prop()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.save_model","text":"Save the whole model folder to a new path. Source code in optuna_hm/handlers/_ecl_simh.py Python def save_model ( self , path , prefix = None ): \"\"\"Save the whole model folder to a new path. Args: path (pathlike) prefix (str, optional) \"\"\" new_folder_name = \"_\" . join if prefix is not None : new_folder_name = f \" { prefix } _ { self . model_name } \" else : new_folder_name = self . model_name save_path = pathlib . Path ( path ) / new_folder_name if not save_path . exists (): save_path . mkdir ( parents = True , exist_ok = True ) for item in self . model_path . iterdir (): if item . is_file (): shutil . copy2 ( item , save_path ) elif item . is_dir (): from distutils.dir_util import copy_tree copy_tree ( str ( item ), str ( save_path / item . name )) logging . info ( f \"Saved model successfully to { str ( save_path ) } \" )","title":"save_model()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.save_model_outputs","text":"Specify model outputs to save for debugging or further review. Parameters: Name Type Description Default files list List of files to save. required outputs list List of files to save. None Source code in optuna_hm/handlers/_ecl_simh.py Python def save_model_outputs ( self , path , files , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: files (list): List of files to save. outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () for file in files : file = pathlib . Path ( file ) name = self . id + \"_\" + file . name shutil . copy2 ( self . model_path / file , path / name )","title":"save_model_outputs()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.set_ecl","text":"Set the simulator application path Source code in optuna_hm/handlers/_ecl_simh.py Python def set_ecl ( self , application_path ): \"\"\"Set the simulator application path\"\"\" self . simexe = application_path if not pathlib . Path ( self . simexe ) . exists (): raise ValueError ( f \"Cannot find simulator executable { self . simexe } \" )","title":"set_ecl()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.set_model_path","text":"Set the path to the model Source code in optuna_hm/handlers/_ecl_simh.py Python def set_model_path ( self , newpath ): \"\"\"Set the path to the model\"\"\" self . model_path = newpath","title":"set_model_path()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.set_save_model_outputs","text":"Specify model outputs to save for debugging or further review. Parameters: Name Type Description Default outputs list List of files to save. None Source code in optuna_hm/handlers/_ecl_simh.py Python def set_save_model_outputs ( self , outputs = None , prefix = None ): \"\"\"Specify model outputs to save for debugging or further review. Args: outputs (list): List of files to save. \"\"\" if prefix is None : prefix = \"\" if outputs is None : shutil . copytree ( self . model_path , prefix + \"_\" + self . id )","title":"set_save_model_outputs()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.set_save_sim_logs","text":"Set location to save model logs. For Eclipse these are the DBG files created by the run. Parameters: Name Type Description Default path str The path to save the logs to. If directory doesn't exist it will be created. required Source code in optuna_hm/handlers/_ecl_simh.py Python def set_save_sim_logs ( self , path ): \"\"\"Set location to save model logs. For Eclipse these are the DBG files created by the run. Args: path (str): The path to save the logs to. If directory doesn't exist it will be created. \"\"\" path = pathlib . Path ( path ) if not path . exists (): path . mkdir () self . log_path = path logging . info ( \"Simulation logs will be saved to: %s \" , str ( path ))","title":"set_save_sim_logs()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLBaseHandler.update_ecl_prop","text":"Creates appropriate property values from random floats in range 0-1 Parameters: Name Type Description Default prop dict list or random floats in range (0-1) supplied by population update required Source code in optuna_hm/handlers/_ecl_simh.py Python def update_ecl_prop ( self , prop ): \"\"\"Creates appropriate property values from random floats in range 0-1 Args: prop (dict): list or random floats in range (0-1) supplied by population update \"\"\" # normalise prop random floats (0-1) to property ranges # this replaces unnormalise if self . original : raise PermissionError ( \"You must create a working copy of the model using\" + \" create_working_model, changes to the original model are not permitted.\" ) self . popt [ \"new_values\" ] = self . popt [ \"NAME\" ] . map ( prop ) for file_name , val in self . popt . groupby ( \"FILE\" ): filepath = os . path . join ( self . model_path , file_name ) updatefile = ECLDatHandler ( filepath ) # change this to write actual update values updatefile . update ( val [[ \"NAME\" , \"new_values\" ]] . set_index ( \"NAME\" )) self . popt = self . popt . drop ( columns = \"new_values\" )","title":"update_ecl_prop()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLDatHandler","text":"Class to update simulation properties based on a comment flag Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLDatHandler : \"\"\" Class to update simulation properties based on a comment flag \"\"\" def __init__ ( self , filename ): \"\"\"\"\"\" logger = logging . getLogger ( __name__ ) try : fdat = open ( filename , \"r\" ) fdat . close () self . filename = filename except FileNotFoundError as err : logger . error ( \"No such ECL DAT file: %s \" , filename ) raise type ( err ) def update ( self , new_values ): \"\"\"Replace objective parameter values with *new_values* in a ECL .DAT file using pre-defined *flags* Args: flags (list): List of parameter flags for new_values new_values (list): List of values to replace where flags Example: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. \"\"\" # check length of flgas and new values - should be the same # load entire file - it is difficult to edit in-place with open ( self . filename , \"r\" ) as fdat : lines = fdat . readlines () # perform edits for i , line in enumerate ( lines ): if line [: 2 ] == \"--\" : pass else : try : for flag in new_values . index : if re . search ( f \"\\s { flag } \\s\" , line ) is not None : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) split [ ind ] = \" {:.4E} \" . format ( new_values [ \"new_values\" ][ flag ]) line = \" \" . join ( split ) + \" \\n \" lines [ i ] = line except ValueError : print ( \"Error updating simulation file.\" ) print ( f \"Problem file: { self . filename } , Problem flag: { flag } \" ) print ( f \"Problem line: { line } \" ) raise SystemExit # write back file with open ( self . filename , \"w\" ) as fdat : fdat . writelines ( lines )","title":"ECLDatHandler"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLDatHandler.update","text":"Replace objective parameter values with new_values in a ECL .DAT file using pre-defined flags Parameters: Name Type Description Default flags list List of parameter flags for new_values required new_values list List of values to replace where flags required Examples: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. Source code in optuna_hm/handlers/_ecl_simh.py Python def update ( self , new_values ): \"\"\"Replace objective parameter values with *new_values* in a ECL .DAT file using pre-defined *flags* Args: flags (list): List of parameter flags for new_values new_values (list): List of values to replace where flags Example: ECL Data File contains EQUALS record like this: EQUALS PERMX -999 1 11 1 11 1 1 / PERMXL1 0 PERMXL2 2 The flag PERMXL1 follows the '/' terminator and fills an ECL control file comment. The integer imediately following the flag tells update which parameter on the line will replaced starting from 0. Here -999 will be replaced. Multiple flags can be used per line such as with PERMXL2 where the first 11 at position 2 will be replaced. If the last value before the '/' terminator is the targeted ensure that there is at least one space ' ' between it the '/' terminator. \"\"\" # check length of flgas and new values - should be the same # load entire file - it is difficult to edit in-place with open ( self . filename , \"r\" ) as fdat : lines = fdat . readlines () # perform edits for i , line in enumerate ( lines ): if line [: 2 ] == \"--\" : pass else : try : for flag in new_values . index : if re . search ( f \"\\s { flag } \\s\" , line ) is not None : split = line . strip () . split () find = split . index ( flag ) ind = int ( split [ find + 1 ]) split [ ind ] = \" {:.4E} \" . format ( new_values [ \"new_values\" ][ flag ]) line = \" \" . join ( split ) + \" \\n \" lines [ i ] = line except ValueError : print ( \"Error updating simulation file.\" ) print ( f \"Problem file: { self . filename } , Problem flag: { flag } \" ) print ( f \"Problem line: { line } \" ) raise SystemExit # write back file with open ( self . filename , \"w\" ) as fdat : fdat . writelines ( lines )","title":"update()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLFakeHandler","text":"Create Fake RSM files to run tests without ECLIPSE Source code in optuna_hm/handlers/_ecl_simh.py Python class ECLFakeHandler ( ECLBaseHandler ): \"\"\"Create Fake RSM files to run tests without ECLIPSE\"\"\" def __init__ ( self , target , reference , * args ): \"\"\"Constructor Args: target (str): column name from handler.popt to reference (pandas.DataFrame): Dataframe containing RSM results of true solution. *args: Arguments for ECLBaseHandler \"\"\" super () . __init__ ( * args ) self . logger = logging . getLogger ( \"ECLFakeHandler\" ) self . target = target self . reference = reference def __deepcopy__ ( self , memo ): # create a copy but not logger return ECLFakeHandler ( copy . deepcopy ( self . target , memo ), copy . deepcopy ( self . reference , memo ), copy . deepcopy ( self . model_name , memo ), copy . deepcopy ( self . model_path , memo ), copy . deepcopy ( self . popt , memo ), ) def clean_results ( self , modelpath , model ): \"\"\"Remove previous results\"\"\" result_file = os . path . join ( modelpath , model , \".RSM\" ) if os . path . isfile ( result_file ): os . remove ( result_file ) self . logger . info ( \"cleaning previous results\" ) def write_rsm ( self , filepath , rsm_well ): \"\"\"Write out a fake RSM file Args: filepath (str): The filepath for the RSM File rsm_well (pandas.DataFrame): \"\"\" with open ( filepath , \"w\" ) as rsmf : rsmf . write ( \" \\n \" ) rsmf . write ( \" \\t SUMMARY OF RUN GENERATED BY ECLFakeHandler \\n \" ) header = [ \" \\t \" ] for col in rsm_well . columns : header += [ col , \" \\t\\t\\t \" ] header += [ \" \\n \" ] * 4 rsmf . write ( \"\" . join ( header )) ncol = len ( rsm_well . columns ) data_template = \" \\t \" + \" {:>8g} \\t\\t \" * ncol for _ , val in rsm_well . iterrows (): rsmf . write ( data_template . format ( * val ) + \" \\n \" ) def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the simulator\"\"\" param = numpy . r_ [ self . retrieve_ecl_prop ()] misfit = ( numpy . sum ( numpy . sqrt ( numpy . square ( param / self . popt [ self . target ] . values - 1 )) ) / param . size ) scalar = 1 + misfit rsm_well = pd . DataFrame () rsm_well [ \"TIME\" ] = self . reference [ \"TIME\" ] for col in self . reference . drop ( columns = \"TIME\" ) . columns : rsm_well [ col + \"H\" ] = self . reference [ col ] rsm_well [ col ] = self . reference [ col ] / scalar filepath = os . path . join ( self . model_path , self . model_name + \".RSM\" ) self . write_rsm ( filepath , rsm_well )","title":"ECLFakeHandler"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLFakeHandler.__init__","text":"Constructor Parameters: Name Type Description Default target str column name from handler.popt to required reference pandas.DataFrame Dataframe containing RSM results of true solution. required *args Arguments for ECLBaseHandler () Source code in optuna_hm/handlers/_ecl_simh.py Python def __init__ ( self , target , reference , * args ): \"\"\"Constructor Args: target (str): column name from handler.popt to reference (pandas.DataFrame): Dataframe containing RSM results of true solution. *args: Arguments for ECLBaseHandler \"\"\" super () . __init__ ( * args ) self . logger = logging . getLogger ( \"ECLFakeHandler\" ) self . target = target self . reference = reference","title":"__init__()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLFakeHandler.clean_results","text":"Remove previous results Source code in optuna_hm/handlers/_ecl_simh.py Python def clean_results ( self , modelpath , model ): \"\"\"Remove previous results\"\"\" result_file = os . path . join ( modelpath , model , \".RSM\" ) if os . path . isfile ( result_file ): os . remove ( result_file ) self . logger . info ( \"cleaning previous results\" )","title":"clean_results()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLFakeHandler.launch","text":"Launch the simulator Source code in optuna_hm/handlers/_ecl_simh.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the simulator\"\"\" param = numpy . r_ [ self . retrieve_ecl_prop ()] misfit = ( numpy . sum ( numpy . sqrt ( numpy . square ( param / self . popt [ self . target ] . values - 1 )) ) / param . size ) scalar = 1 + misfit rsm_well = pd . DataFrame () rsm_well [ \"TIME\" ] = self . reference [ \"TIME\" ] for col in self . reference . drop ( columns = \"TIME\" ) . columns : rsm_well [ col + \"H\" ] = self . reference [ col ] rsm_well [ col ] = self . reference [ col ] / scalar filepath = os . path . join ( self . model_path , self . model_name + \".RSM\" ) self . write_rsm ( filepath , rsm_well )","title":"launch()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_simh.ECLFakeHandler.write_rsm","text":"Write out a fake RSM file Parameters: Name Type Description Default filepath str The filepath for the RSM File required rsm_well pandas.DataFrame required Source code in optuna_hm/handlers/_ecl_simh.py Python def write_rsm ( self , filepath , rsm_well ): \"\"\"Write out a fake RSM file Args: filepath (str): The filepath for the RSM File rsm_well (pandas.DataFrame): \"\"\" with open ( filepath , \"w\" ) as rsmf : rsmf . write ( \" \\n \" ) rsmf . write ( \" \\t SUMMARY OF RUN GENERATED BY ECLFakeHandler \\n \" ) header = [ \" \\t \" ] for col in rsm_well . columns : header += [ col , \" \\t\\t\\t \" ] header += [ \" \\n \" ] * 4 rsmf . write ( \"\" . join ( header )) ncol = len ( rsm_well . columns ) data_template = \" \\t \" + \" {:>8g} \\t\\t \" * ncol for _ , val in rsm_well . iterrows (): rsmf . write ( data_template . format ( * val ) + \" \\n \" ) A compilation of common evaluation functions used for eclipse optimisation projects. Author Tony Hallam 2020","title":"write_rsm()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_eval.Error","text":"Base class for other exceptions Source code in optuna_hm/handlers/_ecl_eval.py Python class Error ( Exception ): \"\"\"Base class for other exceptions\"\"\" def __init__ ( self , message = None ): self . message = message logger . error ( message ) super () . __init__ ()","title":"Error"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_eval.SimulationRunError","text":"Raised when the input value is too small Source code in optuna_hm/handlers/_ecl_eval.py Python class SimulationRunError ( Error ): \"\"\"Raised when the input value is too small\"\"\" pass","title":"SimulationRunError"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_eval.evaluate_eclipse_model","text":"Evaluate an eclipse model. Parameters: Name Type Description Default simulator mophim.eclipse.EclBaseHandler The initialised simulator class object which can be derived from EclBaseHandler required results_dir str, pathlib.Path A directory, the directory must exist. None Source code in optuna_hm/handlers/_ecl_eval.py Python def evaluate_eclipse_model ( simulator , results_dir = None , save_model_run = False , save_summary_results = False , save_properties_results = False , save_model_files = None , sleep = 5 , working_cwd = None , run_sim = True , update_prop = True , summary_results_kwargs = None , properties_results_kwargs = None , ** kwargs , ): \"\"\"Evaluate an eclipse model. Args: simulator (mophim.eclipse.EclBaseHandler): The initialised simulator class object which can be derived from EclBaseHandler results_dir (str, pathlib.Path): A directory, the directory must exist. \"\"\" if not isinstance ( simulator , ECLBaseHandler ): raise ValueError ( f \"simulator argument must be of class { type ( ECLBaseHandler ) } , got { type ( simulator ) } \" ) if results_dir is None : results_dir = pathlib . Path ( \".\" ) . absolute () else : results_dir = pathlib . Path ( results_dir ) . absolute () if not results_dir . is_dir (): raise ValueError ( f \"results_dir must be directory and exist { results_dir } \" ) results = kwargs [ \"results\" ] optimiser = results [ \"sampler\" ] evaluation = results [ \"trial_number\" ] # create working sim logger . info ( f \"Starting { optimiser } trial number { evaluation } \" ) working_simulator = simulator . create_working_model ( working_cwd = working_cwd ) time . sleep ( sleep ) if update_prop : working_simulator . update_ecl_prop ( results [ \"p\" ]) if run_sim : working_simulator . launch () # Wait for simulation finalisation and cleanup time . sleep ( sleep ) # Save model files if requested if save_model_files is not None : model_files_save_path = results_dir / \"model_files\" try : model_files_save_path . mkdir () except FileExistsError : pass working_simulator . save_model_outputs ( model_files_save_path , save_model_files ) model_path_name = ( pathlib . Path ( working_simulator . temp_dir . name ) / working_simulator . model_name ) # Save whole model run if save_model_run : working_simulator . save_model ( results_dir / \"ecl_models\" , prefix = f \" { int ( evaluation ) : 05d } \" ) # read in simulation summary data try : summary_results = { \"well\" : None , \"failed\" : False } if summary_results_kwargs is not None : summary_results = _get_ecl_sumspec_results ( model_path_name , ** summary_results_kwargs ) else : summary_results = _get_ecl_sumspec_results ( model_path_name ) except SimulationOutputError : # simulation failed. logger . warning ( \"Results file not found %s \" , str ( model_path_name )) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} except KeyError as err : # simulation failed. logger . warning ( \"Results keys missing from summary %s \" , err ) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} except SimulationRunError as err : # simulation raised errors - probably failed logger . warning ( \"Simulation run failed. %s \" , err ) summary_results = { \"failed\" : True , \"well\" : None , \"end_date\" : pd . to_datetime ( 0 )} if summary_results [ \"well\" ] is not None and save_summary_results : name = \" {} _ {:04d} _summary.hdf5\" . format ( optimiser , evaluation ) summary_results_path = results_dir / \"summary\" try : summary_results_path . mkdir () except FileExistsError : pass summary_results_path_name = summary_results_path / name summary_results [ \"well\" ] . to_hdf ( summary_results_path_name , \"name\" , mode = \"w\" , format = \"table\" ) del summary_results [ \"well\" ] summary_results [ \"well\" ] = str ( summary_results_path_name ) logger . info ( f \"Summary results hdf5 output { summary_results_path / name } \" ) # read in simulation property data try : eclsim = None if properties_results_kwargs is not None : eclsim = _get_ecl_properties_sub ( model_path_name , ** properties_results_kwargs ) # print(\"eclsim_ret\", eclsim) # except IOError as err: # logger.info(f\"Problems reading property results evaluation {evaluation}\") # print(err) # property_results = {\"eclsim\": eclsim, \"failed\": True} except ( IOError , OSError , SimulationOutputError ) as err : logger . warning ( f \"Problems reading property results evaluation { evaluation } \" ) property_results = { \"eclsim\" : eclsim , \"failed\" : True } except KeyError as err : logger . warning ( f \"Keys not found in results file evaluation { evaluation } \" ) property_results = { \"eclsim\" : eclsim , \"failed\" : True } else : property_results = { \"eclsim\" : eclsim , \"failed\" : False } if property_results [ \"eclsim\" ] is not None and save_properties_results : name = \" {} _ {:04d} _eclsim\" . format ( optimiser , evaluation ) property_results_path = results_dir / \"eclsim\" try : property_results_path . mkdir () except FileExistsError : pass eclsim_save_path_name = property_results_path / name property_results [ \"eclsim\" ] . save ( eclsim_save_path_name ) del property_results [ \"eclsim\" ] property_results [ \"eclsim\" ] = str ( eclsim_save_path_name ) logger . info ( f \"Summary results eclsim output { property_results_path / name } \" ) if summary_results [ \"failed\" ] is True or property_results [ \"failed\" ] is True : failed = True else : failed = False results . update ( summary_results ) results . update ( property_results ) results [ \"failed\" ] = failed results . update ({ \"model_id\" : working_simulator . id }) working_simulator . cleanup ( sleep = sleep ) logger . info ( f \"Eclipse run finished and data loaded, trial number { results [ 'trial_number' ] } \" ) return results","title":"evaluate_eclipse_model()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers._ecl_eval.evaluate_well_prod","text":"Calculated the misfit between all log pairs using method and returns a results dictionary labelled by the first log in the pair. Parameters: Name Type Description Default log_pairs list[(, ), ] A list of length 2 tuples which match labels in the input dataframe passed by the mopphim_kwargs 'results' required date_range tuple(str, str Filter the dataframe based upon a date range. The index of the input dataframe will need to be of type datetime. If the str is empty e.g. \"\" then the filter will be open in that direction. E.g. ('2020-2-02', '') or ('2020-2-02'. '2020-4-04') None columns list List of DataFrame values to perform metric on. required Source code in optuna_hm/handlers/_ecl_eval.py Python def evaluate_well_prod ( log_pairs , date_range = None , method = \"kge\" , data_key = \"well\" , null_fitness = None , ** kwargs , ): \"\"\"Calculated the misfit between all log pairs using method and returns a results dictionary labelled by the first log in the pair. Args: log_pairs (list[(, ), ]): A list of length 2 tuples which match labels in the input dataframe passed by the mopphim_kwargs['results'][data_key] date_range (tuple(str, str)): Filter the dataframe based upon a date range. The index of the input dataframe will need to be of type datetime. If the str is empty e.g. \"\" then the filter will be open in that direction. E.g. ('2020-2-02', '') or ('2020-2-02'. '2020-4-04') columns (list): List of DataFrame values to perform metric on. \"\"\" # set output metrics to null_fitness metrics = { p1 : null_fitness for p1 , _ in log_pairs } if kwargs [ \"results\" ][ \"failed\" ] and kwargs [ \"results\" ][ \"well\" ] is None : return metrics results = kwargs [ \"results\" ] data_path_name = kwargs [ \"results\" ][ data_key ] data = pd . read_hdf ( data_path_name ) available_logs = data . columns output_metrics = dict () if data is None : logger . debug ( \"Missing well results from ECL run. Skipping well evaluation.\" ) return metrics # check log_pairs for log in np . array ( log_pairs ) . flatten (): if not log in available_logs : raise ValueError ( f \"log { log } was not in results\" ) clean_data = data . dropna () if date_range is not None : clean_data = clean_data [ date_range [ 0 ] : date_range [ 1 ]] if clean_data . shape [ 0 ] == 0 : logger . debug ( \"data is empty, check date_range/simulation\" ) return metrics # NOT SURE IF THIS IS STILL NEEDED # no_data = [c for c in plogs if clean_data[c[0]].sum() == 0] # plogs = [c for c in plogs if clean_data[c[0]].sum() > 0] # for c in no_data: # metrics[c[0]] = np.nan if method == \"kge\" : # kling-gupta efficieny for p1 , p2 in log_pairs : output_metrics [ p1 ] = 1 - kge ( clean_data [ p1 ] . values , clean_data [ p2 ] . values , maxfitness = null_fitness , # incase method fails ) # elif method == \"mse\": # scaler = StandardScaler() # data_norm = clean_data.copy() # data_norm.loc[:, :] = scaler.fit_transform(clean_data.values) # for col in plogs: # metrics[col[0]] = mean_squared_error( # data_norm[col[0]].values, data_norm[col[1]].values # ) else : raise ValueError ( f \"Unknown method { method } \" ) logger ( f \"Completed well evaluation, trial_number { results [ 'trial_number' ] } \" ) return output_metrics","title":"evaluate_well_prod()"},{"location":"ophm/api_handlers.html#optuna_hmhandlersflow","text":"Flow is an alternative open-source simulator to Eclipse. It uses similar files as input and output but has a slightly different call signature. This module modifies the Eclipse methods to suit flow where needed. Evaluation and metrics functions from Eclipse can be used with Flow. Handler module for external file and process handling of ECLIPSE flow simulator - Schlumberger hand_ecl100 - Claus Aranha (caranha@cs.tsukuba.ac.jp) hand_ecldat - Tony Hallam","title":"optuna_hm.handlers.flow"},{"location":"ophm/api_handlers.html#optuna_hm.handlers.flow.FlowHandler","text":"modifies ECLBaseHandler to deal with flow. Source code in optuna_hm/handlers/flow.py Python class FlowHandler ( ECLBaseHandler ): \"\"\" modifies ECLBaseHandler to deal with flow. \"\"\" def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"FlowHandler\" ) def __deepcopy__ ( self , memo ): # need to create a special copy function because the logger cannot be coppied # this means reinitialisation is necessary for each new simulator class args = [ self . __dict__ [ var ] for var in [ \"model_name\" , \"model_path\" , \"popt\" , \"simexe\" , \"time_out\" , \"silent\" , ] ] return FlowHandler ( * args ) def clean_results (): # FIXME: delete results file from self.model_path # beginning with self.model_name pass def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the flow simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" model_arg = \"--parameter-file=\" + str ( self . model_name ) + \".PARAM\" self . logger . info ( \"running model at %s \" , self . model_path ) return super () . launch ( model_arg )","title":"FlowHandler"},{"location":"ophm/api_handlers.html#optuna_hm.handlers.flow.FlowHandler.__init__","text":"See ECLBaseHandler Notes: Source code in optuna_hm/handlers/flow.py Python def __init__ ( self , * args , ** kwargs ): \"\"\"See ECLBaseHandler Notes: \"\"\" super () . __init__ ( * args , ** kwargs ) self . logger = logging . getLogger ( \"FlowHandler\" )","title":"__init__()"},{"location":"ophm/api_handlers.html#optuna_hm.handlers.flow.FlowHandler.launch","text":"Launch the flow simulator. Returns: Type Description bool True if run did not timeout. Source code in optuna_hm/handlers/flow.py Python def launch ( self , * cmdargs , ** cmdkwargs ): \"\"\"Launch the flow simulator. Args: None Returns: bool: True if run did not timeout. \"\"\" model_arg = \"--parameter-file=\" + str ( self . model_name ) + \".PARAM\" self . logger . info ( \"running model at %s \" , self . model_path ) return super () . launch ( model_arg )","title":"launch()"},{"location":"ophm/api_metrics.html","text":"Metrics Python from optuna_hm import metrics Functions for evaluation of data differences aka difference metrics This module is a port of internal functions for MOPHiM Vectorised by Tony Hallam 2019 correlation ( m1 , m2 ) Correlation metric Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float metric Source code in optuna_hm/metrics/_metrics.py Python def correlation ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Correlation metric Args: m1: Array 1 m2: Array 2 Returns: metric \"\"\" a = ( m1 - np . mean ( m1 )) / np . std ( m1 ) b = ( m2 - np . mean ( m2 )) / np . std ( m2 ) return 1 - np . abs ( np . sum ( a * b ) / m1 . size ) denom_zdiv ( a , b ) Helper function to avoid divide by zero in many areas. Parameters: Name Type Description Default a array-like Numerator required b array-like Deominator required Returns: Type Description a/b (array-like) Replace div0 by 0 Source code in optuna_hm/metrics/_metrics.py Python def denom_zdiv ( a : np . ndarray , b : np . ndarray ) -> np . ndarray : \"\"\"Helper function to avoid divide by zero in many areas. Args: a (array-like): Numerator b (array-like): Deominator Returns: a/b (array-like): Replace div0 by 0 \"\"\" return np . divide ( a , b , out = np . zeros_like ( b ), where = b != 0.0 ) kendall_tau ( m1 , m2 ) Kendall-Tau Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float metric Source code in optuna_hm/metrics/_metrics.py Python def kendall_tau ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Kendall-Tau Args: m1: Array 1 m2: Array 2 Returns: metric \"\"\" m1 = m1 . flatten () m2 = m2 . flatten () n = m1 . size i_ = np . zeros ( np . sum ( np . arange ( 0 , n )), dtype = int ) j_ = np . zeros_like ( i_ , dtype = int ) k = 0 for i in range ( n ): for j in range ( n - i - 1 ): i_ [ k ] = i j_ [ k ] = j + i + 1 k += 1 nc = np . sum ( np . sign ( m1 [ i_ ] - m1 [ j_ ]) == np . sign ( m2 [ i_ ] - m2 [ j_ ])) nd = k - nc return 1 - ( nc - nd ) / ( n * ( n - 1 ) / 2 ) kge ( m1 , m2 , maxfitness = 100 , s = ( 1 , 1 , 1 ), return_all = False ) COPIED FROM HydroErr Python Package Compute the Kling-Gupta efficiency (2012). Notes : The Range -inf < KGE (2012) < 1, does not indicate bias, larger is better. This is the modified version of the KGE (2009). Kling proposed this version to avoid cross-correlation between bias and variability ratios. Parameters: Name Type Description Default m1 ndarray An array (1D) of simulated data from the time series. required m2 ndarray An array (1D) of observed data from the time series. required s (tuple of length three) Represents the scaling factors to be used for re-scaling the Pearson product-moment correlation coefficient \u00ae, gamma, and Beta, respectively. (1, 1, 1) maxfitness float Return if kge failes. 100 return_all bool If True, returns all of the components of the KGE metric, which are r, gamma, and beta, respectively. False Returns: Type Description float The Kling-Gupta (2012) efficiency value, unless the return_all parameter is True. Refs Kling, H., Fuchs, M., & Paulin, M. (2012). Runoff conditions in the upper Danube basin under an ensemble of climate change scenarios. Journal of Hydrology, 424, 264-277. Source code in optuna_hm/metrics/_metrics.py Python def kge ( m1 : np . ndarray , m2 : np . ndarray , maxfitness : float = 100 , s = ( 1 , 1 , 1 ), return_all : bool = False ) -> float : \"\"\" COPIED FROM HydroErr Python Package Compute the Kling-Gupta efficiency (2012). ***Notes***: - **The Range** -inf < KGE (2012) < 1, does not indicate bias, larger is better. - This is the modified version of the KGE (2009). Kling proposed this version to avoid cross-correlation between bias and variability ratios. Args: m1: An array (1D) of simulated data from the time series. m2: An array (1D) of observed data from the time series. s: (tuple of length three) Represents the scaling factors to be used for re-scaling the Pearson product-moment correlation coefficient (r), gamma, and Beta, respectively. maxfitness: Return if kge failes. return_all: If True, returns all of the components of the KGE metric, which are r, gamma, and beta, respectively. Returns: The Kling-Gupta (2012) efficiency value, unless the return_all parameter is True. Refs: Kling, H., Fuchs, M., & Paulin, M. (2012). Runoff conditions in the upper Danube basin under an ensemble of climate change scenarios. Journal of Hydrology, 424, 264-277. \"\"\" # Means sim_mean = np . nanmean ( m1 ) obs_mean = np . nanmean ( m2 ) m1 = np . where ( np . isnan ( m1 ), sim_mean , m1 ) m2 = np . where ( np . isnan ( m2 ), obs_mean , m2 ) # Standard Deviations sim_sigma = np . std ( m1 ) obs_sigma = np . std ( m2 ) # Pearson R top_pr = np . sum (( m2 - obs_mean ) * ( m1 - sim_mean )) bot1_pr = np . sqrt ( np . sum (( m2 - obs_mean ) ** 2 )) bot2_pr = np . sqrt ( np . sum (( m1 - sim_mean ) ** 2 )) pear_r = top_pr / ( bot1_pr * bot2_pr ) # Ratio between mean of simulated and observed data beta = sim_mean / obs_mean # CV is the coefficient of variation (standard deviation / mean) sim_cv = sim_sigma / sim_mean obs_cv = obs_sigma / obs_mean # Variability Ratio, or the ratio of simulated CV to observed CV gam = sim_cv / obs_cv if obs_mean != 0 and obs_sigma != 0 and sim_mean != 0 : kge_meas = 1 - np . sqrt ( ( s [ 0 ] * ( pear_r - 1 )) ** 2 + ( s [ 1 ] * ( gam - 1 )) ** 2 + ( s [ 2 ] * ( beta - 1 )) ** 2 ) else : kge_meas = - maxfitness if return_all : return pear_r , gam , beta , kge_meas else : return kge_meas minimum_ratio ( m1 , m2 ) Minimum ratio between two arrays m1 and m2 have same shape Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float The disimilarity metric Note While the minimum ratio is a similarity metric this function return (1 - this) for the disimilarity measure. Source code in optuna_hm/metrics/_metrics.py Python def minimum_ratio ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Minimum ratio between two arrays m1 and m2 have same shape Args: m1: Array 1 m2: Array 2 Returns: The disimilarity metric Note: While the minimum ratio is a similarity metric this function return (1 - this) for the disimilarity measure. \"\"\" m1 = m1 . astype ( float ) m2 = m2 . astype ( float ) ret = np . zeros_like ( m1 ) both_zero = np . logical_and ( m1 == 0 , m2 == 0 ) both_nonzero = np . logical_and ( m1 != 0 , m2 != 0 ) ret = np . where ( both_nonzero , np . minimum ( denom_zdiv ( m1 , m2 ), denom_zdiv ( m2 , m1 )), ret ) ret = np . where ( both_zero , 1 , ret ) return 1.0 - ( np . sum ( ret ) / m1 . size ) mse ( m1 , m2 ) Mean square error of two matrices Parameters: Name Type Description Default m1 ndarray Matrix 1 required m2 ndarray Matrix 2 required Returns: Type Description float mse for m1 m2 Source code in optuna_hm/metrics/_metrics.py Python def mse ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Mean square error of two matrices Args: m1: Matrix 1 m2: Matrix 2 Returns: mse for m1 m2 \"\"\" return np . square ( np . subtract ( m1 , m2 )) . mean () norm ( m1 , m2 ) Calculates and returns the differences between two matrices Parameters: Name Type Description Default m1 ndarray Matrix 1 required m2 ndarray Matrix 2 required Returns: Type Description float Normalised difference of m1 and m2 Source code in optuna_hm/metrics/_metrics.py Python def norm ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Calculates and returns the differences between two matrices Args: m1: Matrix 1 m2: Matrix 2 Returns: Normalised difference of m1 and m2 \"\"\" difference = 0 for i , j in zip ( m1 , m2 ): # http://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy ii = np . array ( i ) jj = np . array ( j ) difference += (( ii - jj ) ** 2 ) . mean () return difference pixel_error ( true , test , mode = 'mse' , normalise = True ) Simple pixel bases comparison metric. Other metrics for testing should have this form. Other functions can have multiple kwargs. Parameters: Name Type Description Default true nd.array The truth array. required test nd.array The array to compare to truth. required mode str The mode of comparison for this function. 'mse' Returns: Type Description float The misfit metric. Source code in optuna_hm/metrics/_metrics.py Python def pixel_error ( true , test , mode = \"mse\" , normalise = True ): \"\"\"Simple pixel bases comparison metric. Other metrics for testing should have this form. Other functions can have multiple kwargs. Args: true (nd.array): The truth array. test (nd.array): The array to compare to truth. mode (str): The mode of comparison for this function. Returns: float: The misfit metric. \"\"\" if normalise : std_tru = np . nanstd ( true ) std_tst = np . nanstd ( test ) test = test * ( std_tru / std_tst ) if mode == \"mse\" : return np . nanmean ( np . square ( true - test )) if mode == \"absum\" : return np . nansum ( np . abs ( true - test ))","title":"Metrics"},{"location":"ophm/api_metrics.html#metrics","text":"Python from optuna_hm import metrics Functions for evaluation of data differences aka difference metrics This module is a port of internal functions for MOPHiM Vectorised by Tony Hallam 2019","title":"Metrics"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.correlation","text":"Correlation metric Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float metric Source code in optuna_hm/metrics/_metrics.py Python def correlation ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Correlation metric Args: m1: Array 1 m2: Array 2 Returns: metric \"\"\" a = ( m1 - np . mean ( m1 )) / np . std ( m1 ) b = ( m2 - np . mean ( m2 )) / np . std ( m2 ) return 1 - np . abs ( np . sum ( a * b ) / m1 . size )","title":"correlation()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.denom_zdiv","text":"Helper function to avoid divide by zero in many areas. Parameters: Name Type Description Default a array-like Numerator required b array-like Deominator required Returns: Type Description a/b (array-like) Replace div0 by 0 Source code in optuna_hm/metrics/_metrics.py Python def denom_zdiv ( a : np . ndarray , b : np . ndarray ) -> np . ndarray : \"\"\"Helper function to avoid divide by zero in many areas. Args: a (array-like): Numerator b (array-like): Deominator Returns: a/b (array-like): Replace div0 by 0 \"\"\" return np . divide ( a , b , out = np . zeros_like ( b ), where = b != 0.0 )","title":"denom_zdiv()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.kendall_tau","text":"Kendall-Tau Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float metric Source code in optuna_hm/metrics/_metrics.py Python def kendall_tau ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Kendall-Tau Args: m1: Array 1 m2: Array 2 Returns: metric \"\"\" m1 = m1 . flatten () m2 = m2 . flatten () n = m1 . size i_ = np . zeros ( np . sum ( np . arange ( 0 , n )), dtype = int ) j_ = np . zeros_like ( i_ , dtype = int ) k = 0 for i in range ( n ): for j in range ( n - i - 1 ): i_ [ k ] = i j_ [ k ] = j + i + 1 k += 1 nc = np . sum ( np . sign ( m1 [ i_ ] - m1 [ j_ ]) == np . sign ( m2 [ i_ ] - m2 [ j_ ])) nd = k - nc return 1 - ( nc - nd ) / ( n * ( n - 1 ) / 2 )","title":"kendall_tau()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.kge","text":"COPIED FROM HydroErr Python Package Compute the Kling-Gupta efficiency (2012). Notes : The Range -inf < KGE (2012) < 1, does not indicate bias, larger is better. This is the modified version of the KGE (2009). Kling proposed this version to avoid cross-correlation between bias and variability ratios. Parameters: Name Type Description Default m1 ndarray An array (1D) of simulated data from the time series. required m2 ndarray An array (1D) of observed data from the time series. required s (tuple of length three) Represents the scaling factors to be used for re-scaling the Pearson product-moment correlation coefficient \u00ae, gamma, and Beta, respectively. (1, 1, 1) maxfitness float Return if kge failes. 100 return_all bool If True, returns all of the components of the KGE metric, which are r, gamma, and beta, respectively. False Returns: Type Description float The Kling-Gupta (2012) efficiency value, unless the return_all parameter is True. Refs Kling, H., Fuchs, M., & Paulin, M. (2012). Runoff conditions in the upper Danube basin under an ensemble of climate change scenarios. Journal of Hydrology, 424, 264-277. Source code in optuna_hm/metrics/_metrics.py Python def kge ( m1 : np . ndarray , m2 : np . ndarray , maxfitness : float = 100 , s = ( 1 , 1 , 1 ), return_all : bool = False ) -> float : \"\"\" COPIED FROM HydroErr Python Package Compute the Kling-Gupta efficiency (2012). ***Notes***: - **The Range** -inf < KGE (2012) < 1, does not indicate bias, larger is better. - This is the modified version of the KGE (2009). Kling proposed this version to avoid cross-correlation between bias and variability ratios. Args: m1: An array (1D) of simulated data from the time series. m2: An array (1D) of observed data from the time series. s: (tuple of length three) Represents the scaling factors to be used for re-scaling the Pearson product-moment correlation coefficient (r), gamma, and Beta, respectively. maxfitness: Return if kge failes. return_all: If True, returns all of the components of the KGE metric, which are r, gamma, and beta, respectively. Returns: The Kling-Gupta (2012) efficiency value, unless the return_all parameter is True. Refs: Kling, H., Fuchs, M., & Paulin, M. (2012). Runoff conditions in the upper Danube basin under an ensemble of climate change scenarios. Journal of Hydrology, 424, 264-277. \"\"\" # Means sim_mean = np . nanmean ( m1 ) obs_mean = np . nanmean ( m2 ) m1 = np . where ( np . isnan ( m1 ), sim_mean , m1 ) m2 = np . where ( np . isnan ( m2 ), obs_mean , m2 ) # Standard Deviations sim_sigma = np . std ( m1 ) obs_sigma = np . std ( m2 ) # Pearson R top_pr = np . sum (( m2 - obs_mean ) * ( m1 - sim_mean )) bot1_pr = np . sqrt ( np . sum (( m2 - obs_mean ) ** 2 )) bot2_pr = np . sqrt ( np . sum (( m1 - sim_mean ) ** 2 )) pear_r = top_pr / ( bot1_pr * bot2_pr ) # Ratio between mean of simulated and observed data beta = sim_mean / obs_mean # CV is the coefficient of variation (standard deviation / mean) sim_cv = sim_sigma / sim_mean obs_cv = obs_sigma / obs_mean # Variability Ratio, or the ratio of simulated CV to observed CV gam = sim_cv / obs_cv if obs_mean != 0 and obs_sigma != 0 and sim_mean != 0 : kge_meas = 1 - np . sqrt ( ( s [ 0 ] * ( pear_r - 1 )) ** 2 + ( s [ 1 ] * ( gam - 1 )) ** 2 + ( s [ 2 ] * ( beta - 1 )) ** 2 ) else : kge_meas = - maxfitness if return_all : return pear_r , gam , beta , kge_meas else : return kge_meas","title":"kge()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.minimum_ratio","text":"Minimum ratio between two arrays m1 and m2 have same shape Parameters: Name Type Description Default m1 ndarray Array 1 required m2 ndarray Array 2 required Returns: Type Description float The disimilarity metric Note While the minimum ratio is a similarity metric this function return (1 - this) for the disimilarity measure. Source code in optuna_hm/metrics/_metrics.py Python def minimum_ratio ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Minimum ratio between two arrays m1 and m2 have same shape Args: m1: Array 1 m2: Array 2 Returns: The disimilarity metric Note: While the minimum ratio is a similarity metric this function return (1 - this) for the disimilarity measure. \"\"\" m1 = m1 . astype ( float ) m2 = m2 . astype ( float ) ret = np . zeros_like ( m1 ) both_zero = np . logical_and ( m1 == 0 , m2 == 0 ) both_nonzero = np . logical_and ( m1 != 0 , m2 != 0 ) ret = np . where ( both_nonzero , np . minimum ( denom_zdiv ( m1 , m2 ), denom_zdiv ( m2 , m1 )), ret ) ret = np . where ( both_zero , 1 , ret ) return 1.0 - ( np . sum ( ret ) / m1 . size )","title":"minimum_ratio()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.mse","text":"Mean square error of two matrices Parameters: Name Type Description Default m1 ndarray Matrix 1 required m2 ndarray Matrix 2 required Returns: Type Description float mse for m1 m2 Source code in optuna_hm/metrics/_metrics.py Python def mse ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Mean square error of two matrices Args: m1: Matrix 1 m2: Matrix 2 Returns: mse for m1 m2 \"\"\" return np . square ( np . subtract ( m1 , m2 )) . mean ()","title":"mse()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.norm","text":"Calculates and returns the differences between two matrices Parameters: Name Type Description Default m1 ndarray Matrix 1 required m2 ndarray Matrix 2 required Returns: Type Description float Normalised difference of m1 and m2 Source code in optuna_hm/metrics/_metrics.py Python def norm ( m1 : np . ndarray , m2 : np . ndarray ) -> float : \"\"\"Calculates and returns the differences between two matrices Args: m1: Matrix 1 m2: Matrix 2 Returns: Normalised difference of m1 and m2 \"\"\" difference = 0 for i , j in zip ( m1 , m2 ): # http://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy ii = np . array ( i ) jj = np . array ( j ) difference += (( ii - jj ) ** 2 ) . mean () return difference","title":"norm()"},{"location":"ophm/api_metrics.html#optuna_hm.metrics._metrics.pixel_error","text":"Simple pixel bases comparison metric. Other metrics for testing should have this form. Other functions can have multiple kwargs. Parameters: Name Type Description Default true nd.array The truth array. required test nd.array The array to compare to truth. required mode str The mode of comparison for this function. 'mse' Returns: Type Description float The misfit metric. Source code in optuna_hm/metrics/_metrics.py Python def pixel_error ( true , test , mode = \"mse\" , normalise = True ): \"\"\"Simple pixel bases comparison metric. Other metrics for testing should have this form. Other functions can have multiple kwargs. Args: true (nd.array): The truth array. test (nd.array): The array to compare to truth. mode (str): The mode of comparison for this function. Returns: float: The misfit metric. \"\"\" if normalise : std_tru = np . nanstd ( true ) std_tst = np . nanstd ( test ) test = test * ( std_tru / std_tst ) if mode == \"mse\" : return np . nanmean ( np . square ( true - test )) if mode == \"absum\" : return np . nansum ( np . abs ( true - test ))","title":"pixel_error()"},{"location":"ophm/cli.html","text":"","title":"Cli"},{"location":"ophm/config.html","text":"","title":"Config"},{"location":"ophm/tutorial.html","text":"","title":"Tutorial"},{"location":"opt/cmaes.html","text":"","title":"Cma-Es"},{"location":"opt/intro.html","text":"","title":"Introduction"},{"location":"opt/lexde.html","text":"","title":"Lex-De"},{"location":"sa/intro.html","text":"","title":"Introduction"},{"location":"sa/ovat.html","text":"","title":"OVAT"},{"location":"sa/sobol.html","text":"","title":"Sobol"},{"location":"ug/framework.html","text":"","title":"Optuna-ExternM Framework"}]}